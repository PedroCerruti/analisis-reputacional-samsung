{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìã Caso de Estudio: Metodolog√≠a de Triangulaci√≥n para An√°lisis Reputacional\n",
    "\n",
    "## üéØ Prop√≥sito y Alcance del An√°lisis\n",
    "\n",
    "Este notebook constituye un **caso de estudio metodol√≥gico** dise√±ado para demostrar t√©cnicas de triangulaci√≥n entre an√°lisis exploratorio de datos (EDA) y procesamiento de lenguaje natural (NLP) aplicadas al an√°lisis reputacional en datasets peque√±os.\n",
    "\n",
    "### Contexto y Limitaciones por Dise√±o\n",
    "\n",
    "- **Dataset espec√≠fico**: 1,085 rese√±as de 2 productos (Samsung A15, Motorola G32)\n",
    "- **Alcance temporal**: 2022-2025, mercado argentino, plataforma MercadoLibre  \n",
    "- **Crisis documentadas**: 1 caso principal (Samsung A15 cargador, jul-nov 2024)\n",
    "- **Objetivo**: Demostraci√≥n metodol√≥gica, no sistema productivo\n",
    "\n",
    "### Valor Propuesto del Caso\n",
    "\n",
    "Este an√°lisis **no pretende desarrollar un sistema de detecci√≥n autom√°tica generalizable**, sino demostrar:\n",
    "\n",
    "1. **Metodolog√≠a de triangulaci√≥n** entre t√©cnicas independientes (EDA + NLP)\n",
    "2. **An√°lisis de convergencia** para validar hallazgos en contextos de datos limitados  \n",
    "3. **Framework de diagn√≥stico** que explica causas espec√≠ficas vs predicci√≥n gen√©rica\n",
    "4. **Principios replicables** para an√°lisis similares con limitaciones comparables\n",
    "\n",
    "## üîç Marco Metodol√≥gico\n",
    "\n",
    "### Definici√≥n de Anomal√≠as Reputacionales\n",
    "\n",
    "Para este caso de estudio, definimos **anomal√≠a reputacional** como per√≠odos donde m√∫ltiples indicadores convergen para se√±alar deterioro en la percepci√≥n del producto:\n",
    "\n",
    "**Criterios EDA (An√°lisis Exploratorio):**\n",
    "- Rating promedio mensual < 3.5 (en escala 1-5)\n",
    "- Ca√≠da significativa de rating (> -0.5 puntos per√≠odo a per√≠odo)\n",
    "- Engagement an√≥malo (mediana votos √∫tiles > percentil 75)\n",
    "- Alta volatilidad (desviaci√≥n est√°ndar rating > 1.2)\n",
    "\n",
    "**Criterios NLP (Procesamiento Lenguaje Natural):**\n",
    "- Sentimiento VADER promedio < -0.05 (negativo)\n",
    "- Presencia significativa de vocabulario cr√≠tico (‚â•5 menciones problema espec√≠fico)\n",
    "- Densidad elevada de t√©rminos problem√°ticos en el corpus mensual\n",
    "\n",
    "### Construcci√≥n del Score EDA\n",
    "\n",
    "El **Score EDA** se construye mediante rating directo transformado a escala 0-4 para transparencia metodol√≥gica:\n",
    "\n",
    "```\n",
    "Score = 0: Rating ‚â• 4.0 (Normal)\n",
    "Score = 1: Rating < 4.0 (Baja)  \n",
    "Score = 2: Rating < 3.6 (Moderada)\n",
    "Score = 3: Rating < 3.2 (Alta)\n",
    "Score = 4: Rating < 2.8 (Cr√≠tica)\n",
    "```\n",
    "\n",
    "**Indicadores complementarios** se registran de forma binaria:\n",
    "- Engagement alto: Mediana votos √∫tiles > percentil 75\n",
    "- Ca√≠da significativa: Cambio rating < -0.5 puntos\n",
    "- Alta volatilidad: Desviaci√≥n est√°ndar > 1.2\n",
    "\n",
    "### Metodolog√≠a de Triangulaci√≥n\n",
    "\n",
    "**Principio Central:** En datasets peque√±os, la convergencia entre t√©cnicas independientes aumenta la confianza en los hallazgos m√°s que cualquier t√©cnica aislada.\n",
    "\n",
    "**Proceso de Validaci√≥n Cruzada:**\n",
    "\n",
    "1. **Detecci√≥n EDA**: Identifica per√≠odos an√≥malos mediante m√©tricas cuantitativas\n",
    "2. **Validaci√≥n NLP**: Analiza contenido sem√°ntico de per√≠odos detectados  \n",
    "3. **An√°lisis de Convergencia**: Cuantifica coincidencia entre t√©cnicas\n",
    "4. **Diagn√≥stico Espec√≠fico**: Identifica causas mediante an√°lisis de vocabulario cr√≠tico\n",
    "\n",
    "**Criterios de Convergencia Exitosa:**\n",
    "- Rating cr√≠tico (Score EDA ‚â• 2) Y/O\n",
    "- Sentimiento negativo (VADER < -0.05) Y/O  \n",
    "- Vocabulario problem√°tico (‚â•5 menciones t√©rminos cr√≠ticos)\n",
    "\n",
    "**Umbral de √©xito**: ‚â•2 de 3 criterios cumplidos por per√≠odo\n",
    "\n",
    "### Herramientas de An√°lisis Sem√°ntico\n",
    "\n",
    "**VADER vs TextBlob:** Basado en validaci√≥n emp√≠rica previa (notebook 2_NLP), utilizamos VADER como herramienta principal de an√°lisis de sentimientos debido a su superioridad demostrada para espa√±ol:\n",
    "- VADER: r=0.286 correlaci√≥n con ratings manuales\n",
    "- TextBlob: r=0.091 correlaci√≥n con ratings manuales  \n",
    "- VADER: 22.4% precisi√≥n categ√≥rica vs 20.7% TextBlob\n",
    "\n",
    "**An√°lisis de Vocabulario Cr√≠tico:** Identificaci√≥n autom√°tica de t√©rminos problem√°ticos mediante:\n",
    "- Frecuencia absoluta en corpus mensual\n",
    "- Ranking en top 5 palabras m√°s mencionadas\n",
    "- Filtrado de stopwords b√°sicas para espa√±ol\n",
    "\n",
    "## üéØ Estructura del An√°lisis\n",
    "\n",
    "**Secci√≥n 1:** An√°lisis de Convergencia EDA-NLP  \n",
    "**Secci√≥n 2:** Caracterizaci√≥n Sem√°ntica de Anomal√≠as Detectadas  \n",
    "**Secci√≥n 3:** Caso Samsung A15: Timeline Metodol√≥gico  \n",
    "**Secci√≥n 4:** Triangulaci√≥n y Validaci√≥n Cruzada  \n",
    "\n",
    "---\n",
    "\n",
    "**üìù Nota Metodol√≥gica:** Este enfoque de \"caso de estudio metodol√≥gico\" reconoce expl√≠citamente las limitaciones inherentes del dataset mientras extrae valor m√°ximo para demostraci√≥n t√©cnica y desarrollo de principios replicables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar dataset unificado\n",
    "df = pd.read_csv(\"../data/processed/reviews_unificado.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"üìä Dataset cargado: {len(df)} rese√±as\")\n",
    "print(f\"üìÖ Per√≠odo: {df['date'].min()} a {df['date'].max()}\")\n",
    "print(f\"üè∑Ô∏è Productos: {df['producto'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. üìä An√°lisis de Convergencia EDA-NLP\n",
    "\n",
    "## Objetivo de la Triangulaci√≥n\n",
    "Esta secci√≥n implementa una metodolog√≠a de **triangulaci√≥n t√©cnica** donde \n",
    "comparamos sistem√°ticamente los hallazgos del an√°lisis exploratorio (EDA) \n",
    "con los resultados del procesamiento de lenguaje natural (NLP).\n",
    "\n",
    "## ¬øPor qu√© Triangulaci√≥n en Datasets Peque√±os?\n",
    "Con 1,085 rese√±as y 1 crisis documentada, la validaci√≥n estad√≠stica \n",
    "tradicional es limitada. La convergencia entre t√©cnicas independientes \n",
    "proporciona mayor confianza en los hallazgos espec√≠ficos del caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. AN√ÅLISIS DE CONVERGENCIA EDA-NLP (VERSI√ìN SIMPLIFICADA)\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_rating_convergence(df):\n",
    "    \"\"\"\n",
    "    An√°lisis simplificado basado en rating directo + indicadores b√°sicos transparentes\n",
    "    \"\"\"\n",
    "    anomalias_detectadas = []\n",
    "    \n",
    "    for producto in df['producto'].unique():\n",
    "        product_data = df[df['producto'] == producto].copy()\n",
    "        \n",
    "        # M√©tricas mensuales b√°sicas\n",
    "        monthly_stats = product_data.groupby(product_data['date'].dt.to_period('M')).agg({\n",
    "            'rating': ['mean', 'std', 'count'],\n",
    "            'useful_votes': ['median'],\n",
    "            'text_length': ['mean']\n",
    "        }).round(3)\n",
    "        \n",
    "        # Aplanar columnas\n",
    "        monthly_stats.columns = ['_'.join(col).strip() for col in monthly_stats.columns]\n",
    "        monthly_stats = monthly_stats.reset_index()\n",
    "        monthly_stats.columns = ['periodo', 'rating_mean', 'rating_std', 'rating_count',\n",
    "                                'votes_median', 'length_mean']\n",
    "        \n",
    "        # Calcular cambio temporal\n",
    "        monthly_stats['rating_change'] = monthly_stats['rating_mean'].diff()\n",
    "        \n",
    "        # An√°lisis simplificado por per√≠odo\n",
    "        for idx, row in monthly_stats.iterrows():\n",
    "            \n",
    "            # M√âTRICA PRINCIPAL: Rating directo (transparente)\n",
    "            if row['rating_mean'] < 2.8:\n",
    "                intensidad = \"Cr√≠tica\"\n",
    "                rating_score = 4\n",
    "            elif row['rating_mean'] < 3.2:\n",
    "                intensidad = \"Alta\" \n",
    "                rating_score = 3\n",
    "            elif row['rating_mean'] < 3.6:\n",
    "                intensidad = \"Moderada\"\n",
    "                rating_score = 2\n",
    "            elif row['rating_mean'] < 4.0:\n",
    "                intensidad = \"Baja\"\n",
    "                rating_score = 1\n",
    "            else:\n",
    "                intensidad = \"Normal\"\n",
    "                rating_score = 0\n",
    "            \n",
    "            # INDICADORES COMPLEMENTARIOS (transparentes)\n",
    "            indicadores_activos = []\n",
    "            \n",
    "            # Engagement an√≥malo\n",
    "            if idx > 0:  # Necesitamos baseline\n",
    "                engagement_threshold = monthly_stats['votes_median'].quantile(0.75)\n",
    "                if row['votes_median'] > engagement_threshold:\n",
    "                    indicadores_activos.append(\"Engagement alto\")\n",
    "            \n",
    "            # Ca√≠da significativa\n",
    "            if pd.notna(row['rating_change']) and row['rating_change'] < -0.5:\n",
    "                indicadores_activos.append(\"Ca√≠da significativa\")\n",
    "            \n",
    "            # Volatilidad alta\n",
    "            if row['rating_std'] > 1.2:\n",
    "                indicadores_activos.append(\"Alta volatilidad\")\n",
    "            \n",
    "            # Solo registrar si hay anomal√≠a (rating < 4.0 O indicadores activos)\n",
    "            if rating_score > 0 or len(indicadores_activos) > 0:\n",
    "                anomalias_detectadas.append({\n",
    "                    'producto': producto,\n",
    "                    'periodo': row['periodo'],\n",
    "                    'rating_mean': row['rating_mean'],\n",
    "                    'rating_change': row['rating_change'] if pd.notna(row['rating_change']) else 0,\n",
    "                    'votes_median': row['votes_median'],\n",
    "                    'intensidad': intensidad,\n",
    "                    'rating_score': rating_score,\n",
    "                    'indicadores_complementarios': ', '.join(indicadores_activos) if indicadores_activos else 'Ninguno',\n",
    "                    'num_indicadores': len(indicadores_activos)\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(anomalias_detectadas)\n",
    "\n",
    "# Aplicar an√°lisis simplificado\n",
    "print(\"üìä Ejecutando an√°lisis de convergencia basado en rating directo...\")\n",
    "df_anomalias_rating = analyze_rating_convergence(df)\n",
    "\n",
    "print(f\"\\nüîç Per√≠odos con anomal√≠as detectadas: {len(df_anomalias_rating)}\")\n",
    "print(\"\\nDistribuci√≥n por intensidad:\")\n",
    "if len(df_anomalias_rating) > 0:\n",
    "    print(df_anomalias_rating['intensidad'].value_counts())\n",
    "    \n",
    "    print(f\"\\nüìä AN√ÅLISIS POR PRODUCTO:\")\n",
    "    for producto in df_anomalias_rating['producto'].unique():\n",
    "        producto_data = df_anomalias_rating[df_anomalias_rating['producto'] == producto]\n",
    "        print(f\"\\n{producto}:\")\n",
    "        print(f\"   ‚Ä¢ Per√≠odos an√≥malos: {len(producto_data)}\")\n",
    "        print(f\"   ‚Ä¢ Rating promedio: {producto_data['rating_mean'].mean():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Intensidad predominante: {producto_data['intensidad'].mode().iloc[0]}\")\n",
    "        \n",
    "        # Mostrar per√≠odo m√°s cr√≠tico\n",
    "        periodo_critico = producto_data.loc[producto_data['rating_score'].idxmax()]\n",
    "        print(f\"   ‚Ä¢ Per√≠odo m√°s cr√≠tico: {periodo_critico['periodo']} \"\n",
    "              f\"(Rating: {periodo_critico['rating_mean']:.2f}, \"\n",
    "              f\"Intensidad: {periodo_critico['intensidad']})\")\n",
    "\n",
    "# Mostrar detalle de anomal√≠as Samsung A15\n",
    "print(f\"\\nüìã DETALLE SAMSUNG A15 (CASO DE ESTUDIO):\")\n",
    "samsung_anomalias = df_anomalias_rating[df_anomalias_rating['producto'] == 'Samsung A15']\n",
    "if len(samsung_anomalias) > 0:\n",
    "    print(\"=\"*80)\n",
    "    for _, anomalia in samsung_anomalias.sort_values('periodo').iterrows():\n",
    "        print(f\"{anomalia['periodo']}: Rating {anomalia['rating_mean']:.2f} \"\n",
    "              f\"({anomalia['intensidad']}) - Indicadores: {anomalia['indicadores_complementarios']}\")\n",
    "    \n",
    "    print(f\"\\nüéØ RESUMEN SAMSUNG A15:\")\n",
    "    print(f\"   ‚Ä¢ Total per√≠odos an√≥malos: {len(samsung_anomalias)}\")\n",
    "    print(f\"   ‚Ä¢ Rating m√≠nimo: {samsung_anomalias['rating_mean'].min():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Rating m√°ximo: {samsung_anomalias['rating_mean'].max():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Per√≠odos cr√≠ticos (‚â§3.0): {len(samsung_anomalias[samsung_anomalias['rating_mean'] <= 3.0])}\")\n",
    "\n",
    "print(\"\\n‚úÖ An√°lisis de convergencia EDA completado\")\n",
    "print(\"üìù Metodolog√≠a: Rating directo como m√©trica principal + indicadores complementarios transparentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. üîç Caracterizaci√≥n Sem√°ntica de Anomal√≠as Detectadas\n",
    "\n",
    "## Objetivo del An√°lisis Sem√°ntico\n",
    "Identificar autom√°ticamente las **causas espec√≠ficas** de los per√≠odos \n",
    "an√≥malos detectados en el EDA, superando las limitaciones de m√©tricas \n",
    "cuantitativas simples en contextos de datos limitados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. CARACTERIZACI√ìN SEM√ÅNTICA DE ANOMAL√çAS DETECTADAS (ACTUALIZADA)\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_semantic_patterns_updated(df, anomalias_rating):\n",
    "    \"\"\"\n",
    "    Analiza patrones sem√°nticos en per√≠odos identificados por rating directo\n",
    "    \"\"\"\n",
    "    patrones_semanticos = []\n",
    "    \n",
    "    for _, anomalia in anomalias_rating.iterrows():\n",
    "        producto = anomalia['producto']\n",
    "        periodo = anomalia['periodo']\n",
    "        \n",
    "        # Filtrar rese√±as del per√≠odo an√≥malo identificado por rating\n",
    "        periodo_reviews = df[\n",
    "            (df['producto'] == producto) &\n",
    "            (df['date'].dt.to_period('M') == periodo)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(periodo_reviews) == 0:\n",
    "            continue\n",
    "        \n",
    "        # An√°lisis de sentimientos VADER para triangulaci√≥n con rating directo\n",
    "        from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        def get_vader_sentiment(text):\n",
    "            if pd.isna(text) or text == '':\n",
    "                return 0, 0, 0, 0\n",
    "            try:\n",
    "                scores = sia.polarity_scores(str(text))\n",
    "                return scores['compound'], scores['pos'], scores['neu'], scores['neg']\n",
    "            except:\n",
    "                return 0, 0, 1, 0\n",
    "        \n",
    "        # Aplicar an√°lisis VADER\n",
    "        vader_results = periodo_reviews['text_clean'].apply(get_vader_sentiment)\n",
    "        periodo_reviews['vader_compound'] = [x[0] for x in vader_results]\n",
    "        periodo_reviews['vader_positive'] = [x[1] for x in vader_results]\n",
    "        periodo_reviews['vader_neutral'] = [x[2] for x in vader_results]\n",
    "        periodo_reviews['vader_negative'] = [x[3] for x in vader_results]\n",
    "        \n",
    "        # Extraer vocabulario cr√≠tico para identificaci√≥n de causas espec√≠ficas\n",
    "        periodo_text = ' '.join(periodo_reviews['text_clean'].fillna(''))\n",
    "        words = periodo_text.split()\n",
    "        \n",
    "        # Filtrar stopwords b√°sicas para an√°lisis de contenido\n",
    "        stopwords_basic = ['el', 'la', 'de', 'que', 'y', 'en', 'un', 'es', 'se', 'no', 'te', 'lo', 'le', 'da', 'su', 'por', 'son', 'con', 'para', 'al', 'del', 'los', 'las', 'me', 'mi', 'muy', 'pero', 'si', 'ya', 'o', 'este', 'esta', 'como', 'todo', 'bien', 'mas', 'tiene', 'a']\n",
    "        filtered_words = [w for w in words if len(w) > 2 and w.lower() not in stopwords_basic]\n",
    "        word_freq = Counter(filtered_words)\n",
    "        \n",
    "        # Seleccionar rese√±as m√°s representativas para an√°lisis cualitativo\n",
    "        rese√±as_negativas = periodo_reviews[periodo_reviews['vader_compound'] < -0.05]\n",
    "        if len(rese√±as_negativas) > 0:\n",
    "            rese√±as_representativas = rese√±as_negativas.nlargest(3, 'useful_votes')[['text', 'rating', 'useful_votes']]\n",
    "        else:\n",
    "            rese√±as_representativas = periodo_reviews.nlargest(3, 'useful_votes')[['text', 'rating', 'useful_votes']]\n",
    "        \n",
    "        patrones_semanticos.append({\n",
    "            'producto': producto,\n",
    "            'periodo': periodo,\n",
    "            'n_reviews': len(periodo_reviews),\n",
    "            'avg_rating': periodo_reviews['rating'].mean(),\n",
    "            'avg_sentiment_vader': periodo_reviews['vader_compound'].mean(),\n",
    "            'sentiment_volatility': periodo_reviews['vader_compound'].std(),\n",
    "            'negative_reviews_pct': (periodo_reviews['vader_compound'] < -0.05).mean() * 100,\n",
    "            'positive_reviews_pct': (periodo_reviews['vader_compound'] > 0.05).mean() * 100,\n",
    "            'vocabulario_critico': dict(word_freq.most_common(10)),\n",
    "            'rese√±as_representativas': rese√±as_representativas.to_dict('records'),\n",
    "            # Variables actualizadas para coherencia\n",
    "            'rating_anomalia': anomalia['rating_mean'],  # Rating del per√≠odo an√≥malo\n",
    "            'intensidad': anomalia['intensidad'],        # Cr√≠tica, Alta, Moderada, etc.\n",
    "            'rating_score': anomalia['rating_score'],    # 0-4 basado en rating directo\n",
    "            'indicadores_eda': anomalia['indicadores_complementarios']  # Indicadores adicionales\n",
    "        })\n",
    "    \n",
    "    return patrones_semanticos\n",
    "\n",
    "# Aplicar an√°lisis sem√°ntico actualizado para validaci√≥n cruzada\n",
    "if len(df_anomalias_rating) > 0:\n",
    "    patrones_semanticos_updated = analyze_semantic_patterns_updated(df, df_anomalias_rating)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã CARACTERIZACI√ìN SEM√ÅNTICA DE ANOMAL√çAS (VALIDACI√ìN NLP ACTUALIZADA)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Analizar solo Samsung A15 para el caso de estudio\n",
    "    samsung_patrones = [p for p in patrones_semanticos_updated if p['producto'] == 'Samsung A15']\n",
    "    \n",
    "    print(f\"üéØ AN√ÅLISIS SAMSUNG A15 - TRIANGULACI√ìN RATING + NLP:\")\n",
    "    print(f\"   Total per√≠odos analizados: {len(samsung_patrones)}\")\n",
    "    \n",
    "    for patron in samsung_patrones:\n",
    "        print(f\"\\nüîç PER√çODO: {patron['periodo']}\")\n",
    "        print(f\"   üìä Rating directo: {patron['rating_anomalia']:.2f} ({patron['intensidad']})\")\n",
    "        print(f\"   üìä Rating score: {patron['rating_score']}/4\")\n",
    "        print(f\"   üìà Indicadores EDA: {patron['indicadores_eda']}\")\n",
    "        print(f\"   üìù Rese√±as per√≠odo: {patron['n_reviews']}\")\n",
    "        print(f\"   üí≠ Sentimiento VADER promedio: {patron['avg_sentiment_vader']:.3f}\")\n",
    "        print(f\"   üòü % Rese√±as negativas (VADER): {patron['negative_reviews_pct']:.1f}%\")\n",
    "        print(f\"   üòä % Rese√±as positivas (VADER): {patron['positive_reviews_pct']:.1f}%\")\n",
    "        print(f\"   üî§ Top 3 vocabulario cr√≠tico:\")\n",
    "        top_words = list(patron['vocabulario_critico'].items())[:3]\n",
    "        for word, freq in top_words:\n",
    "            print(f\"      ‚Ä¢ {word}: {freq}\")\n",
    "    \n",
    "    # An√°lisis de convergencia rating directo vs VADER\n",
    "    print(f\"\\nüìä CONVERGENCIA RATING DIRECTO vs VADER:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    convergencias_exitosas = []\n",
    "    for patron in samsung_patrones:\n",
    "        # Criterios de convergencia actualizados para VADER\n",
    "        rating_critico = patron['rating_score'] >= 3  # Alta o Cr√≠tica\n",
    "        vader_negativo = patron['avg_sentiment_vader'] < -0.05  # VADER compound negativo\n",
    "        \n",
    "        # FIX: Convertir dict_keys a lista antes de slicear\n",
    "        vocab_keys = list(patron['vocabulario_critico'].keys())\n",
    "        vocab_problematico = 'cargador' in [w.lower() for w in vocab_keys[:5]]\n",
    "        \n",
    "        # Evaluar convergencia\n",
    "        criterios_met = sum([rating_critico, vader_negativo, vocab_problematico])\n",
    "        \n",
    "        if criterios_met >= 2:  # Al menos 2 de 3 criterios\n",
    "            convergencias_exitosas.append(patron)\n",
    "            print(f\"‚úÖ {patron['periodo']}: CONVERGENCIA EXITOSA\")\n",
    "            print(f\"   ‚Ä¢ Rating: {patron['rating_anomalia']:.2f} ({'‚úì' if rating_critico else '‚úó'})\")\n",
    "            print(f\"   ‚Ä¢ VADER: {patron['avg_sentiment_vader']:.3f} ({'‚úì' if vader_negativo else '‚úó'})\")\n",
    "            print(f\"   ‚Ä¢ 'Cargador' en top 5: {'‚úì' if vocab_problematico else '‚úó'}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {patron['periodo']}: Convergencia parcial ({criterios_met}/3)\")\n",
    "    \n",
    "    print(f\"\\nüéØ RESUMEN DE CONVERGENCIA:\")\n",
    "    print(f\"   ‚Ä¢ Per√≠odos analizados: {len(samsung_patrones)}\")\n",
    "    print(f\"   ‚Ä¢ Convergencias exitosas: {len(convergencias_exitosas)}\")\n",
    "    print(f\"   ‚Ä¢ Tasa de convergencia: {len(convergencias_exitosas)/len(samsung_patrones)*100:.1f}%\")\n",
    "    \n",
    "    # Identificar per√≠odo con mayor convergencia\n",
    "    if convergencias_exitosas:\n",
    "        periodo_mejor = max(convergencias_exitosas, \n",
    "                           key=lambda x: x['rating_score'] + (1 if x['avg_sentiment_vader'] < -0.05 else 0))\n",
    "        print(f\"   ‚Ä¢ Mejor convergencia: {periodo_mejor['periodo']} \"\n",
    "              f\"(Rating: {periodo_mejor['rating_anomalia']:.2f}, \"\n",
    "              f\"VADER: {periodo_mejor['avg_sentiment_vader']:.3f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Caracterizaci√≥n sem√°ntica actualizada completada\")\n",
    "print(\"üìù Metodolog√≠a: Triangulaci√≥n rating directo + an√°lisis VADER (superior a TextBlob)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. üìà Caso Samsung A15: Timeline Metodol√≥gico\n",
    "\n",
    "## Prop√≥sito de la Secci√≥n\n",
    "Demostrar c√≥mo la **triangulaci√≥n metodol√≥gica** proporciona tanto \n",
    "detecci√≥n temporal (EDA) como explicaci√≥n causal (NLP) en un caso \n",
    "espec√≠fico documentado, estableciendo precedente para replicaci√≥n.\n",
    "\n",
    "## Limitaciones del Caso\n",
    "- **Una crisis documentada**: No permite generalizaci√≥n estad√≠stica\n",
    "- **Contexto espec√≠fico**: Argentina, smartphones gama media, MercadoLibre  \n",
    "- **Validaci√≥n post-hoc**: An√°lisis de evento conocido, no predicci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. CASO SAMSUNG A15: TIMELINE METODOL√ìGICO\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_samsung_timeline_updated(df, anomalias_rating, patrones_semanticos):\n",
    "    \"\"\"\n",
    "    Analiza timeline espec√≠fico de Samsung A15 integrando EDA y VADER\n",
    "    \"\"\"\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    \n",
    "    # Filtrar datos Samsung A15\n",
    "    samsung_data = anomalias_rating[anomalias_rating['producto'] == 'Samsung A15'].copy()\n",
    "    samsung_semantics = [p for p in patrones_semanticos if p['producto'] == 'Samsung A15']\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üéØ TIMELINE METODOL√ìGICO SAMSUNG A15\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìã Demostraci√≥n de triangulaci√≥n EDA-VADER en caso espec√≠fico documentado\")\n",
    "    \n",
    "    if len(samsung_data) == 0:\n",
    "        print(\"‚ùå No hay datos de Samsung A15 disponibles\")\n",
    "        return None\n",
    "    \n",
    "    # Ordenar por per√≠odo\n",
    "    samsung_data = samsung_data.sort_values('periodo')\n",
    "    samsung_semantics = sorted(samsung_semantics, key=lambda x: x['periodo'])\n",
    "    \n",
    "    print(f\"\\nüìä PER√çODO DE AN√ÅLISIS: {samsung_data['periodo'].min()} a {samsung_data['periodo'].max()}\")\n",
    "    print(f\"üìà TOTAL PER√çODOS ANALIZADOS: {len(samsung_data)}\")\n",
    "    \n",
    "    # An√°lisis por per√≠odo\n",
    "    print(f\"\\nüîç EVOLUCI√ìN TEMPORAL DETALLADA:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    timeline_data = []\n",
    "    \n",
    "    for i, (_, row) in enumerate(samsung_data.iterrows()):\n",
    "        periodo = row['periodo']\n",
    "        \n",
    "        # Buscar datos sem√°nticos correspondientes\n",
    "        semantic_data = next((s for s in samsung_semantics if s['periodo'] == periodo), None)\n",
    "        \n",
    "        if semantic_data:\n",
    "            # Calcular m√©tricas de convergencia\n",
    "            eda_score = row['rating_score']  # 0-4 basado en rating\n",
    "            vader_score = semantic_data['avg_sentiment_vader']\n",
    "            freq_cargador = semantic_data['vocabulario_critico'].get('cargador', 0)\n",
    "            \n",
    "            # Determinar fase del timeline\n",
    "            if i <= 1:\n",
    "                fase = \"üü¢ INICIAL\"\n",
    "            elif row['rating_mean'] < 3.0:\n",
    "                fase = \"üî¥ CR√çTICA\"\n",
    "            elif row['rating_mean'] < 3.5:\n",
    "                fase = \"üü° DETERIORO\"\n",
    "            else:\n",
    "                fase = \"üîµ ESTABLE\"\n",
    "            \n",
    "            print(f\"\\nüìÖ {periodo} - {fase}\")\n",
    "            print(f\"   üìä Rating: {row['rating_mean']:.2f} ({row['intensidad']})\")\n",
    "            print(f\"   üìä Score EDA: {eda_score}/4\")\n",
    "            print(f\"   üí≠ VADER: {vader_score:.3f}\")\n",
    "            print(f\"   üìù Rese√±as: {semantic_data['n_reviews']}\")\n",
    "            print(f\"   üî§ 'Cargador': {freq_cargador} menciones\")\n",
    "            print(f\"   üòü Negativas VADER: {semantic_data['negative_reviews_pct']:.1f}%\")\n",
    "            print(f\"   üìà Indicadores EDA: {row['indicadores_complementarios']}\")\n",
    "            \n",
    "            # Validaci√≥n cruzada per√≠odo espec√≠fico\n",
    "            convergencia_rating = eda_score >= 2  # Moderada o superior\n",
    "            convergencia_vader = vader_score < -0.05  # Negativo\n",
    "            convergencia_vocab = freq_cargador >= 5  # Menciones significativas\n",
    "            \n",
    "            total_convergencia = sum([convergencia_rating, convergencia_vader, convergencia_vocab])\n",
    "            \n",
    "            if total_convergencia >= 2:\n",
    "                print(f\"   ‚úÖ CONVERGENCIA METODOL√ìGICA: {total_convergencia}/3\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è Convergencia parcial: {total_convergencia}/3\")\n",
    "            \n",
    "            timeline_data.append({\n",
    "                'periodo': periodo,\n",
    "                'rating': row['rating_mean'],\n",
    "                'eda_score': eda_score,\n",
    "                'vader_score': vader_score,\n",
    "                'freq_cargador': freq_cargador,\n",
    "                'convergencia': total_convergencia,\n",
    "                'fase': fase.split()[1]\n",
    "            })\n",
    "    \n",
    "    # An√°lisis de patrones temporales\n",
    "    print(f\"\\nüìà PATRONES TEMPORALES IDENTIFICADOS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Rating m√≠nimo y m√°ximo\n",
    "    min_rating_periodo = samsung_data.loc[samsung_data['rating_mean'].idxmin()]\n",
    "    max_rating_periodo = samsung_data.loc[samsung_data['rating_mean'].idxmax()]\n",
    "    \n",
    "    print(f\"üîª RATING M√çNIMO: {min_rating_periodo['rating_mean']:.2f} en {min_rating_periodo['periodo']}\")\n",
    "    print(f\"üî∫ RATING M√ÅXIMO: {max_rating_periodo['rating_mean']:.2f} en {max_rating_periodo['periodo']}\")\n",
    "    \n",
    "    # VADER m√°s negativo\n",
    "    vader_min = min(samsung_semantics, key=lambda x: x['avg_sentiment_vader'])\n",
    "    print(f\"üò∞ VADER M√ÅS NEGATIVO: {vader_min['avg_sentiment_vader']:.3f} en {vader_min['periodo']}\")\n",
    "    \n",
    "    # Pico de \"cargador\"\n",
    "    cargador_max = max(samsung_semantics, key=lambda x: x['vocabulario_critico'].get('cargador', 0))\n",
    "    print(f\"üî§ PICO 'CARGADOR': {cargador_max['vocabulario_critico'].get('cargador', 0)} menciones en {cargador_max['periodo']}\")\n",
    "    \n",
    "    # An√°lisis de desfases temporales\n",
    "    print(f\"\\nüïê DESFASES TEMPORALES OBSERVADOS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Comparar per√≠odos cr√≠ticos\n",
    "    periodo_min_rating = min_rating_periodo['periodo']\n",
    "    periodo_max_cargador = cargador_max['periodo']\n",
    "    periodo_min_vader = vader_min['periodo']\n",
    "    \n",
    "    print(f\"üìä Pico vocabulario cr√≠tico: {periodo_max_cargador}\")\n",
    "    print(f\"üìä Rating m√≠nimo: {periodo_min_rating}\")  \n",
    "    print(f\"üìä VADER m√°s negativo: {periodo_min_vader}\")\n",
    "    \n",
    "    if periodo_max_cargador != periodo_min_rating:\n",
    "        print(f\"üí° DESFASE IDENTIFICADO: Vocabulario cr√≠tico precede a rating m√≠nimo\")\n",
    "        print(f\"   ‚Üí Indica que NLP puede actuar como indicador adelantado\")\n",
    "    else:\n",
    "        print(f\"üí° COINCIDENCIA TEMPORAL: Vocabulario y rating cr√≠ticos simult√°neos\")\n",
    "    \n",
    "    # Resumen de validaci√≥n metodol√≥gica\n",
    "    print(f\"\\nüéØ VALIDACI√ìN METODOL√ìGICA:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    convergencias_exitosas = sum(1 for data in timeline_data if data['convergencia'] >= 2)\n",
    "    tasa_convergencia = convergencias_exitosas / len(timeline_data) * 100\n",
    "    \n",
    "    print(f\"üìä Per√≠odos con convergencia EDA-VADER: {convergencias_exitosas}/{len(timeline_data)}\")\n",
    "    print(f\"üìä Tasa de convergencia metodol√≥gica: {tasa_convergencia:.1f}%\")\n",
    "    \n",
    "    # Identificar patrones espec√≠ficos\n",
    "    rating_trend = \"DETERIORO ‚Üí RECUPERACI√ìN\" if samsung_data['rating_mean'].iloc[-1] > samsung_data['rating_mean'].min() else \"DETERIORO SOSTENIDO\"\n",
    "    print(f\"üìà Tendencia general: {rating_trend}\")\n",
    "    \n",
    "    # Consistencia del problema \"cargador\"\n",
    "    periodos_con_cargador = sum(1 for s in samsung_semantics if s['vocabulario_critico'].get('cargador', 0) >= 3)\n",
    "    print(f\"üî§ Per√≠odos con problema 'cargador': {periodos_con_cargador}/{len(samsung_semantics)}\")\n",
    "    print(f\"üìã Consistencia del problema espec√≠fico: {periodos_con_cargador/len(samsung_semantics)*100:.1f}%\")\n",
    "    \n",
    "    return timeline_data\n",
    "\n",
    "# Ejecutar an√°lisis de timeline\n",
    "if len(df_anomalias_rating) > 0 and 'patrones_semanticos_updated' in locals():\n",
    "    print(\"üîÑ Ejecutando an√°lisis de timeline Samsung A15...\")\n",
    "    timeline_results = analyze_samsung_timeline_updated(df, df_anomalias_rating, patrones_semanticos_updated)\n",
    "    \n",
    "    if timeline_results:\n",
    "        print(f\"\\n‚úÖ Timeline metodol√≥gico completado\")\n",
    "        print(f\"üìù Metodolog√≠a: Integraci√≥n EDA + VADER para validaci√≥n cruzada temporal\")\n",
    "        \n",
    "        # Guardar resultados para uso posterior\n",
    "        timeline_df = pd.DataFrame(timeline_results)\n",
    "        print(f\"üìä Timeline data generado: {len(timeline_df)} per√≠odos procesados\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error en generaci√≥n de timeline\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Requisitos no cumplidos para an√°lisis de timeline\")\n",
    "    print(f\"   - df_anomalias_rating: {'‚úì' if len(df_anomalias_rating) > 0 else '‚úó'}\")\n",
    "    print(f\"   - patrones_semanticos_updated: {'‚úì' if 'patrones_semanticos_updated' in locals() else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. üîó Triangulaci√≥n y Validaci√≥n Cruzada\n",
    "\n",
    "## Metodolog√≠a de Convergencia\n",
    "An√°lisis sistem√°tico de coincidencias y discrepancias entre EDA y NLP \n",
    "para establecer **confidence intervals** en contexto de datos limitados.\n",
    "\n",
    "## M√©tricas de Validaci√≥n Cruzada\n",
    "- Per√≠odos donde ambas t√©cnicas coinciden en detectar anomal√≠as\n",
    "- Grado de especificidad en identificaci√≥n de causas\n",
    "- Robustez de hallazgos bajo diferentes aproximaciones metodol√≥gicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triangulation_dashboard_complete_with_vader(df, anomalias_rating, patrones_semanticos):\n",
    "    \"\"\"\n",
    "    Dashboard completo de triangulaci√≥n metodol√≥gica EDA-VADER-Engagement (4 variables)\n",
    "    Incluye an√°lisis de sentimiento VADER como cuarta dimensi√≥n\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from matplotlib.dates import DateFormatter\n",
    "    import matplotlib.dates as mdates\n",
    "    \n",
    "    # Configurar estilo\n",
    "    plt.style.use('default')\n",
    "    sns.set_style(\"dark\")\n",
    "    \n",
    "    # Filtrar datos Samsung A15\n",
    "    samsung_eda = anomalias_rating[anomalias_rating['producto'] == 'Samsung A15'].copy()\n",
    "    samsung_nlp = [p for p in patrones_semanticos if p['producto'] == 'Samsung A15']\n",
    "    \n",
    "    if len(samsung_eda) == 0 or len(samsung_nlp) == 0:\n",
    "        print(\"‚ùå Datos insuficientes para triangulaci√≥n\")\n",
    "        return None\n",
    "    \n",
    "    # Crear DataFrame consolidado para visualizaci√≥n\n",
    "    triangulation_data = []\n",
    "    \n",
    "    for _, row in samsung_eda.iterrows():\n",
    "        periodo = row['periodo']\n",
    "        \n",
    "        # Buscar datos NLP correspondientes\n",
    "        nlp_data = next((s for s in samsung_nlp if s['periodo'] == periodo), None)\n",
    "        \n",
    "        if nlp_data:\n",
    "            triangulation_data.append({\n",
    "                'periodo': periodo,\n",
    "                'rating_mean': row['rating_mean'],\n",
    "                'votes_median': row['votes_median'],\n",
    "                'freq_cargador': nlp_data['vocabulario_critico'].get('cargador', 0),\n",
    "                'vader_sentiment': nlp_data['avg_sentiment_vader'],\n",
    "                'rating_score': row['rating_score'],\n",
    "                'intensidad': row['intensidad']\n",
    "            })\n",
    "    \n",
    "    df_triangulation = pd.DataFrame(triangulation_data)\n",
    "    df_triangulation['periodo_dt'] = pd.to_datetime(df_triangulation['periodo'].astype(str))\n",
    "    df_triangulation = df_triangulation.sort_values('periodo_dt')\n",
    "    \n",
    "    # Crear gr√°fico de triangulaci√≥n completa con 4 variables\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    \n",
    "    # Colores por intensidad de rating\n",
    "    color_map = {\n",
    "        'Normal': '#2E8B57',      # Verde\n",
    "        'Baja': '#FFD700',        # Dorado  \n",
    "        'Moderada': '#FF8C00',    # Naranja\n",
    "        'Alta': '#FF4500',        # Naranja rojizo\n",
    "        'Cr√≠tica': '#DC143C'      # Rojo\n",
    "    }\n",
    "    \n",
    "    # 1. Barras de rating con colores por intensidad (Variable Principal)\n",
    "    colors = [color_map.get(intensidad, '#808080') for intensidad in df_triangulation['intensidad']]\n",
    "    bars = ax.bar(df_triangulation['periodo_dt'], df_triangulation['rating_mean'], \n",
    "                  color=colors, alpha=0.7, label='Rating Promedio (EDA)', width=25, \n",
    "                  edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # A√±adir valores en las barras\n",
    "    for bar, rating in zip(bars, df_triangulation['rating_mean']):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{rating:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # 2. L√≠nea de frecuencia \"cargador\" (NLP)\n",
    "    ax2 = ax.twinx()\n",
    "    line_nlp = ax2.plot(df_triangulation['periodo_dt'], df_triangulation['freq_cargador'], \n",
    "                        'ro-', linewidth=4, markersize=10, label='Frecuencia \"Cargador\" (NLP)', \n",
    "                        alpha=0.9, markeredgecolor='darkmagenta', markeredgewidth=1)\n",
    "    \n",
    "    # 3. L√≠nea de mediana engagement (con eje separado)\n",
    "    ax3 = ax.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    line_eng = ax3.plot(df_triangulation['periodo_dt'], df_triangulation['votes_median'], \n",
    "                        's--', color='purple', linewidth=3, markersize=8, \n",
    "                        label='Mediana Engagement', alpha=0.8,\n",
    "                        markeredgecolor='indigo', markeredgewidth=1)\n",
    "    \n",
    "    # 4. L√≠nea de an√°lisis de sentimiento VADER (con eje separado)\n",
    "    ax4 = ax.twinx()\n",
    "    ax4.spines['right'].set_position(('outward', 120))\n",
    "    line_vader = ax4.plot(df_triangulation['periodo_dt'], df_triangulation['vader_sentiment'], \n",
    "                          '^-', color='darkblue', linewidth=3, markersize=8, \n",
    "                          label='VADER Sentiment', alpha=0.9,\n",
    "                          markeredgecolor='navy', markeredgewidth=1)\n",
    "    \n",
    "    # Configurar ejes y etiquetas\n",
    "    ax.set_ylabel('Rating Promedio (EDA)', fontsize=13, fontweight='bold', color='black')\n",
    "    ax2.set_ylabel('Frecuencia \"Cargador\" (NLP)', fontsize=13, fontweight='bold', color='red')\n",
    "    ax3.set_ylabel('Mediana Votos √ötiles (Engagement)', fontsize=12, fontweight='bold', color='purple')\n",
    "    ax4.set_ylabel('VADER Sentiment Analysis', fontsize=12, fontweight='bold', color='darkblue')\n",
    "    \n",
    "    ax.set_xlabel('Per√≠odo', fontsize=13, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelcolor='black', labelsize=11)\n",
    "    ax2.tick_params(axis='y', labelcolor='red', labelsize=11)\n",
    "    ax3.tick_params(axis='y', labelcolor='purple', labelsize=11)\n",
    "    ax4.tick_params(axis='y', labelcolor='darkblue', labelsize=11)\n",
    "    \n",
    "    # Formatear fechas en eje X\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%Y-%m'))\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, fontsize=11)\n",
    "    \n",
    "    # T√≠tulo principal\n",
    "    ax.set_title('Triangulaci√≥n Metodol√≥gica Completa: Samsung A15\\n' + \n",
    "                 'EDA + NLP + Engagement + VADER Sentiment Analysis (4 Dimensiones)', \n",
    "                 fontsize=16, fontweight='bold', pad=25)\n",
    "    \n",
    "    # L√≠neas de referencia\n",
    "    fecha = df_triangulation['periodo_dt'].iloc[-1]\n",
    "    # L√≠nea cr√≠tica en rating 3.0\n",
    "    ax.axhline(y=3.0, color='darkred', linestyle=':', alpha=0.8, linewidth=2)\n",
    "    ax.text(fecha, 3.1, 'Umbral Cr√≠tico', \n",
    "            fontsize=10, color='darkred', fontweight='bold', ha='right', va='bottom')\n",
    "    \n",
    "    # L√≠nea neutral en VADER sentiment (0.0)\n",
    "    ax4.axhline(y=0.0, color='darkblue', linestyle=':', alpha=0.6, linewidth=2)\n",
    "    ax4.text(fecha, 0.02, 'Sentiment Neutral', \n",
    "             fontsize=10, color='darkblue', fontweight='bold', ha='right', va='bottom')\n",
    "    \n",
    "    # Leyenda consolidada - posicionada a la altura de 2025-01\n",
    "    lines1, labels1 = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels() \n",
    "    lines3, labels3 = ax3.get_legend_handles_labels()\n",
    "    lines4, labels4 = ax4.get_legend_handles_labels()\n",
    "    \n",
    "    # Encontrar la posici√≥n de 2025-01 o el per√≠odo m√°s cercano\n",
    "    target_date = pd.to_datetime('2025-01')\n",
    "    if target_date in df_triangulation['periodo_dt'].values:\n",
    "        legend_x_pos = target_date\n",
    "    else:\n",
    "        # Usar el primer per√≠odo de 2025 disponible o el √∫ltimo si no existe\n",
    "        dates_2025 = df_triangulation[df_triangulation['periodo_dt'].dt.year == 2025]['periodo_dt']\n",
    "        if len(dates_2025) > 0:\n",
    "            legend_x_pos = dates_2025.iloc[0]\n",
    "        else:\n",
    "            legend_x_pos = df_triangulation['periodo_dt'].iloc[-1]\n",
    "    \n",
    "    # Leyenda principal de variables posicionada manualmente\n",
    "    legend_variables = ax.legend(lines1 + lines2 + lines3 + lines4, \n",
    "                                labels1 + labels2 + labels3 + labels4, \n",
    "                                loc='center right', fontsize=10, framealpha=0.9,\n",
    "                                title='Variables de Triangulaci√≥n', title_fontsize=11,\n",
    "                                bbox_to_anchor=(0.9, 0.92))\n",
    "    \n",
    "    # Leyenda de intensidad de rating\n",
    "    intensity_elements = [plt.Rectangle((0,0),1,1, color=color_map[intensity], alpha=0.8) \n",
    "                         for intensity in ['Normal', 'Baja', 'Moderada', 'Alta', 'Cr√≠tica']]\n",
    "    intensity_labels = ['Normal', 'Baja', 'Moderada', 'Alta', 'Cr√≠tica']\n",
    "    \n",
    "    legend_intensity = ax.legend(intensity_elements, intensity_labels, \n",
    "                                title='Intensidad Rating', loc='upper right', \n",
    "                                fontsize=9, title_fontsize=10, framealpha=0.9)\n",
    "    ax.add_artist(legend_variables)  # Mantener ambas leyendas\n",
    "    \n",
    "    # Zona de destacado para per√≠odo cr√≠tico\n",
    "    if len(df_triangulation[df_triangulation['rating_mean'] < 3.0]) > 0:\n",
    "        periodos_criticos = df_triangulation[df_triangulation['rating_mean'] < 3.0]\n",
    "        for _, periodo_critico in periodos_criticos.iterrows():\n",
    "            ax.axvspan(periodo_critico['periodo_dt'] - pd.Timedelta(days=10), \n",
    "                      periodo_critico['periodo_dt'] + pd.Timedelta(days=10), \n",
    "                      alpha=0.1, color='red', label='Per√≠odo Cr√≠tico' if _ == periodos_criticos.index[0] else \"\")\n",
    "    \n",
    "    # Grid y formato final\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar gr√°fico\n",
    "    plt.savefig('../outputs/visualizations/03_grafico_triangulacion_completa.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Resumen estad√≠stico de triangulaci√≥n\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä RESUMEN DE TRIANGULACI√ìN METODOL√ìGICA (4 VARIABLES)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üéØ PRODUCTO ANALIZADO: Samsung A15\")\n",
    "    print(f\"üìÖ PER√çODO: {df_triangulation['periodo'].min()} - {df_triangulation['periodo'].max()}\")\n",
    "    print(f\"üìà PER√çODOS TOTALES: {len(df_triangulation)}\")\n",
    "    \n",
    "    print(f\"\\nüìã M√âTRICAS POR VARIABLE:\")\n",
    "    print(f\"   ‚Ä¢ Rating promedio general: {df_triangulation['rating_mean'].mean():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Rating m√≠nimo registrado: {df_triangulation['rating_mean'].min():.2f} ({df_triangulation.loc[df_triangulation['rating_mean'].idxmin(), 'periodo']})\")\n",
    "    print(f\"   ‚Ä¢ M√°xima frecuencia 'Cargador': {df_triangulation['freq_cargador'].max()} ({df_triangulation.loc[df_triangulation['freq_cargador'].idxmax(), 'periodo']})\")\n",
    "    print(f\"   ‚Ä¢ VADER m√°s negativo: {df_triangulation['vader_sentiment'].min():.3f} ({df_triangulation.loc[df_triangulation['vader_sentiment'].idxmin(), 'periodo']})\")\n",
    "    print(f\"   ‚Ä¢ Engagement m√°ximo: {df_triangulation['votes_median'].max()} votos ({df_triangulation.loc[df_triangulation['votes_median'].idxmax(), 'periodo']})\")\n",
    "    \n",
    "    # An√°lisis de correlaciones\n",
    "    correlaciones = {}\n",
    "    correlaciones['Rating-VADER'] = df_triangulation['rating_mean'].corr(df_triangulation['vader_sentiment'])\n",
    "    correlaciones['Rating-Cargador'] = df_triangulation['rating_mean'].corr(df_triangulation['freq_cargador'])\n",
    "    correlaciones['Rating-Engagement'] = df_triangulation['rating_mean'].corr(df_triangulation['votes_median'])\n",
    "    correlaciones['VADER-Cargador'] = df_triangulation['vader_sentiment'].corr(df_triangulation['freq_cargador'])\n",
    "    \n",
    "    print(f\"\\nüîó CORRELACIONES ENTRE VARIABLES:\")\n",
    "    for par, corr in correlaciones.items():\n",
    "        signo = \"üìà\" if corr > 0 else \"üìâ\"\n",
    "        fuerza = \"Fuerte\" if abs(corr) > 0.7 else \"Moderada\" if abs(corr) > 0.4 else \"D√©bil\"\n",
    "        print(f\"   {signo} {par}: {corr:.3f} ({fuerza})\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dashboard de triangulaci√≥n completa generado:\")\n",
    "    print(f\"   üìä Archivo: triangulacion_completa_4_variables_vader.png\")\n",
    "    print(f\"   üî¨ Metodolog√≠a: EDA + NLP + Engagement + VADER Sentiment\")\n",
    "    print(f\"   üìê T√©cnica de Sentimiento: VADER (Valence Aware Dictionary and sEntiment Reasoner)\")\n",
    "    \n",
    "    return df_triangulation\n",
    "\n",
    "# Funci√≥n para ejecutar si existen los datos necesarios\n",
    "def execute_complete_triangulation():\n",
    "    \"\"\"\n",
    "    Ejecuta la triangulaci√≥n completa con verificaci√≥n de requisitos\n",
    "    \"\"\"\n",
    "    required_vars = ['df_anomalias_rating', 'patrones_semanticos_updated', 'df']\n",
    "    missing_vars = [var for var in required_vars if var not in locals() and var not in globals()]\n",
    "    \n",
    "    if missing_vars:\n",
    "        print(f\"‚ö†Ô∏è Variables requeridas faltantes: {missing_vars}\")\n",
    "        print(f\"   Ejecuta primero las secciones anteriores del notebook\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üîÑ Iniciando triangulaci√≥n metodol√≥gica completa...\")\n",
    "    triangulation_data = create_triangulation_dashboard_complete_with_vader(\n",
    "        df, df_anomalias_rating, patrones_semanticos_updated\n",
    "    )\n",
    "    \n",
    "    return triangulation_data\n",
    "\n",
    "triangulation_data = execute_complete_triangulation()\n",
    "\n",
    "if 'df_anomalias_rating' in locals() and 'patrones_semanticos_updated' in locals():\n",
    "    print(\"üîÑ Ejecutando triangulaci√≥n metodol√≥gica completa...\")\n",
    "    triangulation_data = create_triangulation_dashboard_complete_with_vader(\n",
    "        df, df_anomalias_rating, patrones_semanticos_updated\n",
    "    )\n",
    "    print(f\"‚úÖ Variable triangulation_data creada con {len(triangulation_data) if triangulation_data is not None else 0} registros\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Variables requeridas no encontradas\")\n",
    "    triangulation_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report_graphics(triangulation_df):\n",
    "    \"\"\"\n",
    "    Genera dos gr√°ficos espec√≠ficos para el informe:\n",
    "    1. Rating + \"Cargador\" (mensaje central)\n",
    "    2. Rating + VADER + Engagement (an√°lisis profundo)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "    from matplotlib.dates import DateFormatter\n",
    "    import matplotlib.dates as mdates\n",
    "    \n",
    "    # Configurar estilo para informe\n",
    "    plt.style.use('default')\n",
    "    sns.set_style(\"dark\")\n",
    "    \n",
    "    # Colores por intensidad\n",
    "    color_map = {\n",
    "        'Normal': '#2E8B57',      # Verde\n",
    "        'Baja': '#FFD700',        # Dorado  \n",
    "        'Moderada': '#FF8C00',    # Naranja\n",
    "        'Alta': '#FF4500',        # Naranja rojizo\n",
    "        'Cr√≠tica': '#DC143C'      # Rojo\n",
    "    }\n",
    "    \n",
    "    colors = [color_map.get(intensidad, '#808080') for intensidad in triangulation_df['intensidad']]\n",
    "    \n",
    "    # =============================================================================\n",
    "    # GR√ÅFICO 1: RATING + \"CARGADOR\" (MENSAJE CENTRAL)\n",
    "    # =============================================================================\n",
    "    \n",
    "    fig1, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Barras de rating con colores por intensidad\n",
    "    bars1 = ax1.bar(triangulation_df['periodo_dt'], triangulation_df['rating_mean'], \n",
    "                    color=colors, alpha=0.6, width=25, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # A√±adir valores en las barras\n",
    "    for bar, rating in zip(bars1, triangulation_df['rating_mean']):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{rating:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    # L√≠nea de frecuencia \"cargador\"\n",
    "    ax2_1 = ax1.twinx()\n",
    "    line_cargador = ax2_1.plot(triangulation_df['periodo_dt'], triangulation_df['freq_cargador'], \n",
    "                               'mo-', linewidth=4, markersize=10, label='Menciones \"Cargador\"', \n",
    "                               alpha=0.9, markeredgecolor='darkmagenta', markeredgewidth=1)\n",
    "    \n",
    "    # Configurar ejes\n",
    "    ax1.set_ylabel('Rating Promedio', fontsize=14, fontweight='bold', color='black')\n",
    "    ax2_1.set_ylabel('Menciones \"Cargador\" por Per√≠odo', fontsize=14, fontweight='bold', color='darkmagenta')\n",
    "    ax1.set_xlabel('Per√≠odo', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax1.tick_params(axis='y', labelcolor='black', labelsize=12)\n",
    "    ax2_1.tick_params(axis='y', labelcolor='darkred', labelsize=12)\n",
    "    \n",
    "    # Formatear fechas\n",
    "    ax1.xaxis.set_major_formatter(DateFormatter('%Y-%m'))\n",
    "    ax1.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, fontsize=11)\n",
    "    \n",
    "    # T√≠tulo y l√≠neas de referencia\n",
    "    ax1.set_title('Identificaci√≥n Autom√°tica del Problema del Cargador\\nCaso Samsung A15: Validaci√≥n Cruzada de T√©cnicas', \n",
    "                  fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # L√≠nea cr√≠tica en rating 3.0 (alineada a la izquierda)\n",
    "    ax1.axhline(y=3.0, color='darkblue', linestyle=':', alpha=0.8, linewidth=2)\n",
    "    ax1.text(ax1.get_xlim()[1], 3.1, 'Umbral Cr√≠tico', \n",
    "         fontsize=11, color='darkblue', fontweight='bold', ha='right', va='bottom')\n",
    "    ax1.grid(False)\n",
    "    \n",
    "    # Leyenda (esquina superior derecha)\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2_1.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right', fontsize=12, framealpha=0.9)\n",
    "    \n",
    "    # Formato\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/visualizations/03_gr√°fico_informe_triangulacion1_rating_cargador.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # =============================================================================\n",
    "    # GR√ÅFICO 2: RATING + AN√ÅLISIS DE SENTIMIENTO + ENGAGEMENT (AN√ÅLISIS PROFUNDO)\n",
    "    # =============================================================================\n",
    "    \n",
    "    fig2, ax1_2 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Barras de rating TRANSPARENTES como fondo/contexto\n",
    "    bars2 = ax1_2.bar(triangulation_df['periodo_dt'], triangulation_df['rating_mean'], \n",
    "                      color=colors, alpha=0.25, width=25, \n",
    "                      edgecolor='gray', linewidth=0.8)\n",
    "    \n",
    "    # L√≠nea de an√°lisis de sentimiento (protagonista)\n",
    "    ax2_2 = ax1_2.twinx()\n",
    "    line_sentiment = ax2_2.plot(triangulation_df['periodo_dt'], triangulation_df['vader_sentiment'], \n",
    "                               'b^-', linewidth=4, markersize=10, label='An√°lisis de Sentimiento', \n",
    "                               alpha=0.9, markeredgecolor='darkblue', markeredgewidth=1)\n",
    "    \n",
    "    # L√≠nea de engagement (protagonista)\n",
    "    ax3_2 = ax1_2.twinx()\n",
    "    ax3_2.spines['right'].set_position(('outward', 60))\n",
    "    line_engagement = ax3_2.plot(triangulation_df['periodo_dt'], triangulation_df['votes_median'], \n",
    "                                 's--', color='purple', linewidth=3, markersize=8, \n",
    "                                 label='Engagement (Votos √ötiles)', alpha=0.9,\n",
    "                                 markeredgecolor='indigo', markeredgewidth=1)\n",
    "    \n",
    "    # Configurar ejes\n",
    "    ax1_2.set_ylabel('Rating Promedio (referencia)', fontsize=13, fontweight='bold', color='darkgray')\n",
    "    ax2_2.set_ylabel('An√°lisis de Sentimiento', fontsize=14, fontweight='bold', color='darkblue')\n",
    "    ax3_2.set_ylabel('Engagement (Votos √ötiles)', fontsize=14, fontweight='bold', color='purple')\n",
    "    ax1_2.set_xlabel('Per√≠odo', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax1_2.tick_params(axis='y', labelcolor='gray', labelsize=11)\n",
    "    ax2_2.tick_params(axis='y', labelcolor='darkblue', labelsize=12)\n",
    "    ax3_2.tick_params(axis='y', labelcolor='purple', labelsize=12)\n",
    "    \n",
    "    # Formatear fechas\n",
    "    ax1_2.xaxis.set_major_formatter(DateFormatter('%Y-%m'))\n",
    "    ax1_2.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.setp(ax1_2.xaxis.get_majorticklabels(), rotation=45, fontsize=11)\n",
    "    \n",
    "    # T√≠tulo\n",
    "    ax1_2.set_title('An√°lisis Emocional y de Respuesta Social\\nCaso Samsung A15: Indicadores Multidimensionales', \n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # L√≠neas de referencia (alineadas a la izquierda)\n",
    "    ax2_2.axhline(y=0, color='darkblue', linestyle=':', alpha=0.6, linewidth=2)\n",
    "    ax2_2.text(ax1_2.get_xlim()[1], 0.02, 'Sentimiento Neutral', \n",
    "           fontsize=11, color='darkblue', fontweight='bold', ha='right', va='bottom')\n",
    "    \n",
    "    # Zona cr√≠tica para sentimiento\n",
    "    ax2_2.axhspan(-0.3, -0.05, alpha=0.1, color='red', label='Zona Negativa')\n",
    "    \n",
    "    # Leyenda consolidada (esquina superior derecha)\n",
    "    lines1, labels1 = ax1_2.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2_2.get_legend_handles_labels()\n",
    "    lines3, labels3 = ax3_2.get_legend_handles_labels()\n",
    "    ax1_2.legend(lines1 + lines2 + lines3, labels1 + labels2 + labels3, \n",
    "                 loc='upper right', fontsize=11, framealpha=0.9)\n",
    "    \n",
    "   \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/visualizations/03_gr√°fico_informe_trinagulaci√≥n2_rating_sentiment_engagement.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Gr√°ficos para informe generados:\")\n",
    "    print(\"   üìä Gr√°fico 1: informe_grafico1_rating_cargador.png\")\n",
    "    print(\"   üìä Gr√°fico 2: informe_grafico2_rating_sentiment_engagement.png\")\n",
    "    print(\"   üí° Versi√≥n adaptada para audiencia no t√©cnica\")\n",
    "\n",
    "# Ejecutar generaci√≥n de gr√°ficos para informe\n",
    "if 'triangulation_data' in locals() and triangulation_data is not None:\n",
    "    print(\"üîÑ Generando gr√°ficos espec√≠ficos para informe...\")\n",
    "    create_report_graphics(triangulation_data)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Ejecuta primero la Secci√≥n 4 completa para generar triangulation_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Conclusiones: S√≠ntesis de Resultados Metodol√≥gicos\n",
    "\n",
    "## üìä Resultados Principales del Caso de Estudio\n",
    "\n",
    "### Convergencia Metodol√≥gica Exitosa\n",
    "\n",
    "El an√°lisis de triangulaci√≥n EDA-NLP demostr√≥ una **convergencia metodol√≥gica del 80.0%** en el caso Samsung A15, validando la efectividad del framework para datasets peque√±os.\n",
    "\n",
    "**¬øQu√© significa convergencia metodol√≥gica?** Es el grado en que t√©cnicas independientes (EDA cuantitativo, an√°lisis sem√°ntico NLP, y vocabulario cr√≠tico) coinciden en identificar los mismos per√≠odos como problem√°ticos. Una alta convergencia indica que m√∫ltiples enfoques anal√≠ticos est√°n \"viendo\" el mismo fen√≥meno, aumentando la confianza en los hallazgos cuando el dataset es peque√±o para validaci√≥n estad√≠stica robusta.\n",
    "\n",
    "**Resultados cuantitativos:**\n",
    "- **Per√≠odos analizados**: 10 (junio 2024 - marzo 2025)\n",
    "- **Convergencias exitosas**: 8/10 per√≠odos (‚â•2/3 criterios cumplidos)\n",
    "- **Performance por criterio individual**:\n",
    "  - VADER negativo (<-0.05): **90% detecci√≥n** (9/10 per√≠odos)\n",
    "  - Vocabulario cr√≠tico \"cargador\" (‚â•5): **80% detecci√≥n** (8/10 per√≠odos) \n",
    "  - Rating cr√≠tico (‚â•Moderada): **70% detecci√≥n** (7/10 per√≠odos)\n",
    "\n",
    "### Identificaci√≥n Exitosa de Problema Espec√≠fico\n",
    "\n",
    "El framework identific√≥ autom√°ticamente el **problema del cargador** como causa central de la crisis Samsung A15:\n",
    "\n",
    "- **Consistencia temporal**: Presente en 8/10 per√≠odos analizados (80%)\n",
    "- **Pico de menciones**: 31 referencias en octubre 2024\n",
    "- **Persistencia**: Problema sostenido julio 2024 - marzo 2025\n",
    "- **Especificidad**: Causa √∫nica y focalizada vs crisis multifactorial\n",
    "\n",
    "## üïê Desfases Temporales Identificados\n",
    "\n",
    "### NLP Como Indicador Adelantado Confirmado\n",
    "\n",
    "El an√°lisis revel√≥ un **patr√≥n consistente de desfase temporal** entre t√©cnicas:\n",
    "\n",
    "**Secuencia Temporal Observada:**\n",
    "1. **Julio-Septiembre 2024**: Descenso abrupto VADER (0.300 ‚Üí -0.221) anticipando crisis\n",
    "2. **Octubre 2024**: Pico vocabulario cr√≠tico (31 menciones \"cargador\")\n",
    "3. **Noviembre 2024**: Rating m√≠nimo (2.70) + Score EDA m√°ximo (4/4)\n",
    "4. **Enero 2025**: VADER alcanza punto m√°s negativo (-0.247) post-impacto\n",
    "\n",
    "**Interpretaci√≥n Metodol√≥gica:**\n",
    "- **VADER anticipa** el deterioro desde julio, detectando el problema 3-4 meses antes del m√≠nimo rating\n",
    "- **Vocabulario cr√≠tico** identifica la causa espec√≠fica 1 mes antes del impacto m√°ximo\n",
    "- **Rating confirma** el deterioro cuando ya es evidente en m√©tricas cuantitativas\n",
    "- **VADER contin√∫a** profundizando emocionalmente incluso durante la recuperaci√≥n inicial\n",
    "\n",
    "### Valor del Desfase para An√°lisis Futuro\n",
    "\n",
    "Este patr√≥n sugiere que **el an√°lisis sem√°ntico puede actuar como indicador adelantado** de deterioro reputacional, proporcionando ventana temporal para intervenci√≥n antes de que el impacto se materialice completamente en ratings promedio.\n",
    "\n",
    "## üìà Lecciones Metodol√≥gicas Clave\n",
    "\n",
    "### Fortalezas Demostradas del Framework\n",
    "\n",
    "**1. Triangulaci√≥n Robusta en Datasets Peque√±os**\n",
    "- 80% convergencia con solo 10 per√≠odos analizados\n",
    "- M√∫ltiples t√©cnicas independientes validan hallazgos\n",
    "- Reduce dependencia de validaci√≥n estad√≠stica tradicional\n",
    "\n",
    "**2. Diagn√≥stico Espec√≠fico Automatizado**\n",
    "- Identificaci√≥n de causas ra√≠z sin an√°lisis manual previo\n",
    "- Vocabulario cr√≠tico emerge autom√°ticamente del corpus\n",
    "- Especificidad que supera m√©tricas agregadas simples\n",
    "\n",
    "## üöÄ Aplicabilidad y Replicabilidad\n",
    "\n",
    "### Condiciones y Adaptaciones para Replicaci√≥n\n",
    "\n",
    "**Contextos Apropiados:**\n",
    "- Datasets peque√±os-medianos (5-50 per√≠odos temporales)\n",
    "- Productos con vocabulario cr√≠tico identificable\n",
    "- Crisis focalizadas vs difusas multifactoriales\n",
    "\n",
    "**Adaptaciones Requeridas:**\n",
    "- Calibraci√≥n de umbrales por mercado/contexto espec√≠fico\n",
    "- Selecci√≥n de herramientas NLP apropiadas para idioma target\n",
    "- Definici√≥n de vocabulario cr√≠tico relevante por industria\n",
    "\n",
    "**Extensiones Metodol√≥gicas Propuestas:**\n",
    "- Validaci√≥n cruzada con m√∫ltiples crisis documentadas\n",
    "- Sistema de alertas basado en umbral de convergencia\n",
    "- Integraci√≥n con fuentes de datos adicionales\n",
    "\n",
    "## üí° Contribuciones al Estado del Arte\n",
    "\n",
    "### Valor Metodol√≥gico del Framework\n",
    "\n",
    "**1. Triangulaci√≥n en Contextos de Datos Limitados**\n",
    "- Demuestra efectividad de validaci√≥n cruzada con muestras peque√±as\n",
    "- Principios transferibles a otras aplicaciones de an√°lisis reputacional\n",
    "- Framework flexible adaptable a diferentes contextos e industrias\n",
    "\n",
    "**2. Integraci√≥n Pr√°ctica EDA-NLP**\n",
    "- Combina simplicidad interpretativa con sofisticaci√≥n t√©cnica\n",
    "- Balance entre automatizaci√≥n y capacidad de diagn√≥stico espec√≠fico\n",
    "- Metodolog√≠a escalable manteniendo principios fundamentales\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ S√≠ntesis Final:** Este caso de estudio demuestra que la triangulaci√≥n metodol√≥gica puede extraer insights v√°lidos y accionables de datasets peque√±os, proporcionando tanto detecci√≥n de anomal√≠as como diagn√≥stico espec√≠fico de causas, siempre que se reconozcan expl√≠citamente las limitaciones contextuales y se dise√±en expectativas realistas apropiadas para el alcance disponible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
