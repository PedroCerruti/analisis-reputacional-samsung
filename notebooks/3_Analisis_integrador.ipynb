{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìã Caso de Estudio: Metodolog√≠a de Triangulaci√≥n para An√°lisis Reputacional\n",
    "\n",
    "## üéØ Prop√≥sito y Alcance del An√°lisis\n",
    "\n",
    "Este notebook constituye un **caso de estudio metodol√≥gico** dise√±ado para demostrar t√©cnicas de triangulaci√≥n entre an√°lisis exploratorio de datos (EDA) y procesamiento de lenguaje natural (NLP) aplicadas al an√°lisis reputacional en datasets peque√±os.\n",
    "\n",
    "### Contexto y Limitaciones por Dise√±o\n",
    "\n",
    "- **Dataset espec√≠fico**: 1,085 rese√±as de 2 productos (Samsung A15, Motorola G32)\n",
    "- **Alcance temporal**: 2022-2025, mercado argentino, plataforma MercadoLibre  \n",
    "- **Crisis documentadas**: 1 caso principal (Samsung A15 cargador, jul-nov 2024)\n",
    "- **Objetivo**: Demostraci√≥n metodol√≥gica, no sistema productivo\n",
    "\n",
    "### Valor Propuesto del Caso\n",
    "\n",
    "Este an√°lisis **no pretende desarrollar un sistema de detecci√≥n autom√°tica generalizable**, sino demostrar:\n",
    "\n",
    "1. **Metodolog√≠a de triangulaci√≥n** entre t√©cnicas independientes (EDA + NLP)\n",
    "2. **An√°lisis de convergencia** para validar hallazgos en contextos de datos limitados  \n",
    "3. **Framework de diagn√≥stico** que explica causas espec√≠ficas vs predicci√≥n gen√©rica\n",
    "4. **Principios replicables** para an√°lisis similares con limitaciones comparables\n",
    "\n",
    "## üîç Marco Metodol√≥gico\n",
    "\n",
    "### Definici√≥n de Anomal√≠as Reputacionales\n",
    "\n",
    "Para este caso de estudio, definimos **anomal√≠a reputacional** como per√≠odos donde m√∫ltiples indicadores convergen para se√±alar deterioro en la percepci√≥n del producto:\n",
    "\n",
    "**Criterios EDA (An√°lisis Exploratorio):**\n",
    "- Rating promedio mensual < 3.5 (en escala 1-5)\n",
    "- Ca√≠da significativa de rating (> -0.5 puntos per√≠odo a per√≠odo)\n",
    "- Engagement an√≥malo (mediana votos √∫tiles > percentil 75)\n",
    "- Alta volatilidad (desviaci√≥n est√°ndar rating > 1.2)\n",
    "\n",
    "**Criterios NLP (Procesamiento Lenguaje Natural):**\n",
    "- Sentimiento VADER promedio < -0.05 (negativo)\n",
    "- Presencia significativa de vocabulario cr√≠tico (‚â•5 menciones problema espec√≠fico)\n",
    "- Densidad elevada de t√©rminos problem√°ticos en el corpus mensual\n",
    "\n",
    "### Construcci√≥n del Score EDA\n",
    "\n",
    "El **Score EDA** se construye mediante rating directo transformado a escala 0-4 para transparencia metodol√≥gica:\n",
    "\n",
    "```\n",
    "Score = 0: Rating ‚â• 4.0 (Normal)\n",
    "Score = 1: Rating < 4.0 (Baja)  \n",
    "Score = 2: Rating < 3.6 (Moderada)\n",
    "Score = 3: Rating < 3.2 (Alta)\n",
    "Score = 4: Rating < 2.8 (Cr√≠tica)\n",
    "```\n",
    "\n",
    "**Indicadores complementarios** se registran de forma binaria:\n",
    "- Engagement alto: Mediana votos √∫tiles > percentil 75\n",
    "- Ca√≠da significativa: Cambio rating < -0.5 puntos\n",
    "- Alta volatilidad: Desviaci√≥n est√°ndar > 1.2\n",
    "\n",
    "### Metodolog√≠a de Triangulaci√≥n\n",
    "\n",
    "**Principio Central:** En datasets peque√±os, la convergencia entre t√©cnicas independientes aumenta la confianza en los hallazgos m√°s que cualquier t√©cnica aislada.\n",
    "\n",
    "**Proceso de Validaci√≥n Cruzada:**\n",
    "\n",
    "1. **Detecci√≥n EDA**: Identifica per√≠odos an√≥malos mediante m√©tricas cuantitativas\n",
    "2. **Validaci√≥n NLP**: Analiza contenido sem√°ntico de per√≠odos detectados  \n",
    "3. **An√°lisis de Convergencia**: Cuantifica coincidencia entre t√©cnicas\n",
    "4. **Diagn√≥stico Espec√≠fico**: Identifica causas mediante an√°lisis de vocabulario cr√≠tico\n",
    "\n",
    "**Criterios de Convergencia Exitosa:**\n",
    "- Rating cr√≠tico (Score EDA ‚â• 2) Y/O\n",
    "- Sentimiento negativo (VADER < -0.05) Y/O  \n",
    "- Vocabulario problem√°tico (‚â•5 menciones t√©rminos cr√≠ticos)\n",
    "\n",
    "**Umbral de √©xito**: ‚â•2 de 3 criterios cumplidos por per√≠odo\n",
    "\n",
    "### Herramientas de An√°lisis Sem√°ntico\n",
    "\n",
    "**VADER vs TextBlob:** Basado en validaci√≥n emp√≠rica previa (notebook 2_NLP), utilizamos VADER como herramienta principal de an√°lisis de sentimientos debido a su superioridad demostrada para espa√±ol:\n",
    "- VADER: r=0.286 correlaci√≥n con ratings manuales\n",
    "- TextBlob: r=0.091 correlaci√≥n con ratings manuales  \n",
    "- VADER: 22.4% precisi√≥n categ√≥rica vs 20.7% TextBlob\n",
    "\n",
    "**An√°lisis de Vocabulario Cr√≠tico:** Identificaci√≥n autom√°tica de t√©rminos problem√°ticos mediante:\n",
    "- Frecuencia absoluta en corpus mensual\n",
    "- Ranking en top 5 palabras m√°s mencionadas\n",
    "- Filtrado de stopwords b√°sicas para espa√±ol\n",
    "\n",
    "## üéØ Estructura del An√°lisis\n",
    "\n",
    "**Secci√≥n 1:** An√°lisis de Convergencia EDA-NLP  \n",
    "**Secci√≥n 2:** Caracterizaci√≥n Sem√°ntica de Anomal√≠as Detectadas  \n",
    "**Secci√≥n 3:** Caso Samsung A15: Timeline Metodol√≥gico  \n",
    "**Secci√≥n 4:** Triangulaci√≥n y Validaci√≥n Cruzada  \n",
    "\n",
    "---\n",
    "\n",
    "**üìù Nota Metodol√≥gica:** Este enfoque de \"caso de estudio metodol√≥gico\" reconoce expl√≠citamente las limitaciones inherentes del dataset mientras extrae valor m√°ximo para demostraci√≥n t√©cnica y desarrollo de principios replicables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar dataset unificado\n",
    "df = pd.read_csv(\"../data/processed/reviews_unificado.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"üìä Dataset cargado: {len(df)} rese√±as\")\n",
    "print(f\"üìÖ Per√≠odo: {df['date'].min()} a {df['date'].max()}\")\n",
    "print(f\"üè∑Ô∏è Productos: {df['producto'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. üìä An√°lisis de Convergencia EDA-NLP\n",
    "\n",
    "## Objetivo de la Triangulaci√≥n\n",
    "Esta secci√≥n implementa una metodolog√≠a de **triangulaci√≥n t√©cnica** donde \n",
    "comparamos sistem√°ticamente los hallazgos del an√°lisis exploratorio (EDA) \n",
    "con los resultados del procesamiento de lenguaje natural (NLP).\n",
    "\n",
    "## ¬øPor qu√© Triangulaci√≥n en Datasets Peque√±os?\n",
    "Con 1,085 rese√±as y 1 crisis documentada, la validaci√≥n estad√≠stica \n",
    "tradicional es limitada. La convergencia entre t√©cnicas independientes \n",
    "proporciona mayor confianza en los hallazgos espec√≠ficos del caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. AN√ÅLISIS DE CONVERGENCIA EDA-NLP (VERSI√ìN SIMPLIFICADA)\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_rating_convergence(df):\n",
    "    \"\"\"\n",
    "    An√°lisis simplificado basado en rating directo + indicadores b√°sicos transparentes\n",
    "    \"\"\"\n",
    "    anomalias_detectadas = []\n",
    "    \n",
    "    for producto in df['producto'].unique():\n",
    "        product_data = df[df['producto'] == producto].copy()\n",
    "        \n",
    "        # M√©tricas mensuales b√°sicas\n",
    "        monthly_stats = product_data.groupby(product_data['date'].dt.to_period('M')).agg({\n",
    "            'rating': ['mean', 'std', 'count'],\n",
    "            'useful_votes': ['median'],\n",
    "            'text_length': ['mean']\n",
    "        }).round(3)\n",
    "        \n",
    "        # Aplanar columnas\n",
    "        monthly_stats.columns = ['_'.join(col).strip() for col in monthly_stats.columns]\n",
    "        monthly_stats = monthly_stats.reset_index()\n",
    "        monthly_stats.columns = ['periodo', 'rating_mean', 'rating_std', 'rating_count',\n",
    "                                'votes_median', 'length_mean']\n",
    "        \n",
    "        # Calcular cambio temporal\n",
    "        monthly_stats['rating_change'] = monthly_stats['rating_mean'].diff()\n",
    "        \n",
    "        # An√°lisis simplificado por per√≠odo\n",
    "        for idx, row in monthly_stats.iterrows():\n",
    "            \n",
    "            # M√âTRICA PRINCIPAL: Rating directo (transparente)\n",
    "            if row['rating_mean'] < 2.8:\n",
    "                intensidad = \"Cr√≠tica\"\n",
    "                rating_score = 4\n",
    "            elif row['rating_mean'] < 3.2:\n",
    "                intensidad = \"Alta\" \n",
    "                rating_score = 3\n",
    "            elif row['rating_mean'] < 3.6:\n",
    "                intensidad = \"Moderada\"\n",
    "                rating_score = 2\n",
    "            elif row['rating_mean'] < 4.0:\n",
    "                intensidad = \"Baja\"\n",
    "                rating_score = 1\n",
    "            else:\n",
    "                intensidad = \"Normal\"\n",
    "                rating_score = 0\n",
    "            \n",
    "            # INDICADORES COMPLEMENTARIOS (transparentes)\n",
    "            indicadores_activos = []\n",
    "            \n",
    "            # Engagement an√≥malo\n",
    "            if idx > 0:  # Necesitamos baseline\n",
    "                engagement_threshold = monthly_stats['votes_median'].quantile(0.75)\n",
    "                if row['votes_median'] > engagement_threshold:\n",
    "                    indicadores_activos.append(\"Engagement alto\")\n",
    "            \n",
    "            # Ca√≠da significativa\n",
    "            if pd.notna(row['rating_change']) and row['rating_change'] < -0.5:\n",
    "                indicadores_activos.append(\"Ca√≠da significativa\")\n",
    "            \n",
    "            # Volatilidad alta\n",
    "            if row['rating_std'] > 1.2:\n",
    "                indicadores_activos.append(\"Alta volatilidad\")\n",
    "            \n",
    "            # Solo registrar si hay anomal√≠a (rating < 4.0 O indicadores activos)\n",
    "            if rating_score > 0 or len(indicadores_activos) > 0:\n",
    "                anomalias_detectadas.append({\n",
    "                    'producto': producto,\n",
    "                    'periodo': row['periodo'],\n",
    "                    'rating_mean': row['rating_mean'],\n",
    "                    'rating_change': row['rating_change'] if pd.notna(row['rating_change']) else 0,\n",
    "                    'votes_median': row['votes_median'],\n",
    "                    'intensidad': intensidad,\n",
    "                    'rating_score': rating_score,\n",
    "                    'indicadores_complementarios': ', '.join(indicadores_activos) if indicadores_activos else 'Ninguno',\n",
    "                    'num_indicadores': len(indicadores_activos)\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(anomalias_detectadas)\n",
    "\n",
    "# Aplicar an√°lisis simplificado\n",
    "print(\"üìä Ejecutando an√°lisis de convergencia basado en rating directo...\")\n",
    "df_anomalias_rating = analyze_rating_convergence(df)\n",
    "\n",
    "print(f\"\\nüîç Per√≠odos con anomal√≠as detectadas: {len(df_anomalias_rating)}\")\n",
    "print(\"\\nDistribuci√≥n por intensidad:\")\n",
    "if len(df_anomalias_rating) > 0:\n",
    "    print(df_anomalias_rating['intensidad'].value_counts())\n",
    "    \n",
    "    print(f\"\\nüìä AN√ÅLISIS POR PRODUCTO:\")\n",
    "    for producto in df_anomalias_rating['producto'].unique():\n",
    "        producto_data = df_anomalias_rating[df_anomalias_rating['producto'] == producto]\n",
    "        print(f\"\\n{producto}:\")\n",
    "        print(f\"   ‚Ä¢ Per√≠odos an√≥malos: {len(producto_data)}\")\n",
    "        print(f\"   ‚Ä¢ Rating promedio: {producto_data['rating_mean'].mean():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Intensidad predominante: {producto_data['intensidad'].mode().iloc[0]}\")\n",
    "        \n",
    "        # Mostrar per√≠odo m√°s cr√≠tico\n",
    "        periodo_critico = producto_data.loc[producto_data['rating_score'].idxmax()]\n",
    "        print(f\"   ‚Ä¢ Per√≠odo m√°s cr√≠tico: {periodo_critico['periodo']} \"\n",
    "              f\"(Rating: {periodo_critico['rating_mean']:.2f}, \"\n",
    "              f\"Intensidad: {periodo_critico['intensidad']})\")\n",
    "\n",
    "# Mostrar detalle de anomal√≠as Samsung A15\n",
    "print(f\"\\nüìã DETALLE SAMSUNG A15 (CASO DE ESTUDIO):\")\n",
    "samsung_anomalias = df_anomalias_rating[df_anomalias_rating['producto'] == 'Samsung A15']\n",
    "if len(samsung_anomalias) > 0:\n",
    "    print(\"=\"*80)\n",
    "    for _, anomalia in samsung_anomalias.sort_values('periodo').iterrows():\n",
    "        print(f\"{anomalia['periodo']}: Rating {anomalia['rating_mean']:.2f} \"\n",
    "              f\"({anomalia['intensidad']}) - Indicadores: {anomalia['indicadores_complementarios']}\")\n",
    "    \n",
    "    print(f\"\\nüéØ RESUMEN SAMSUNG A15:\")\n",
    "    print(f\"   ‚Ä¢ Total per√≠odos an√≥malos: {len(samsung_anomalias)}\")\n",
    "    print(f\"   ‚Ä¢ Rating m√≠nimo: {samsung_anomalias['rating_mean'].min():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Rating m√°ximo: {samsung_anomalias['rating_mean'].max():.2f}\")\n",
    "    print(f\"   ‚Ä¢ Per√≠odos cr√≠ticos (‚â§3.0): {len(samsung_anomalias[samsung_anomalias['rating_mean'] <= 3.0])}\")\n",
    "\n",
    "print(\"\\n‚úÖ An√°lisis de convergencia EDA completado\")\n",
    "print(\"üìù Metodolog√≠a: Rating directo como m√©trica principal + indicadores complementarios transparentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. üîç Caracterizaci√≥n Sem√°ntica de Anomal√≠as Detectadas\n",
    "\n",
    "## Objetivo del An√°lisis Sem√°ntico\n",
    "Identificar autom√°ticamente las **causas espec√≠ficas** de los per√≠odos \n",
    "an√≥malos detectados en el EDA, superando las limitaciones de m√©tricas \n",
    "cuantitativas simples en contextos de datos limitados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. CARACTERIZACI√ìN SEM√ÅNTICA DE ANOMAL√çAS DETECTADAS (ACTUALIZADA)\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_semantic_patterns_updated(df, anomalias_rating):\n",
    "    \"\"\"\n",
    "    Analiza patrones sem√°nticos en per√≠odos identificados por rating directo\n",
    "    \"\"\"\n",
    "    patrones_semanticos = []\n",
    "    \n",
    "    for _, anomalia in anomalias_rating.iterrows():\n",
    "        producto = anomalia['producto']\n",
    "        periodo = anomalia['periodo']\n",
    "        \n",
    "        # Filtrar rese√±as del per√≠odo an√≥malo identificado por rating\n",
    "        periodo_reviews = df[\n",
    "            (df['producto'] == producto) &\n",
    "            (df['date'].dt.to_period('M') == periodo)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(periodo_reviews) == 0:\n",
    "            continue\n",
    "        \n",
    "        # An√°lisis de sentimientos VADER para triangulaci√≥n con rating directo\n",
    "        from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        def get_vader_sentiment(text):\n",
    "            if pd.isna(text) or text == '':\n",
    "                return 0, 0, 0, 0\n",
    "            try:\n",
    "                scores = sia.polarity_scores(str(text))\n",
    "                return scores['compound'], scores['pos'], scores['neu'], scores['neg']\n",
    "            except:\n",
    "                return 0, 0, 1, 0\n",
    "        \n",
    "        # Aplicar an√°lisis VADER\n",
    "        vader_results = periodo_reviews['text_clean'].apply(get_vader_sentiment)\n",
    "        periodo_reviews['vader_compound'] = [x[0] for x in vader_results]\n",
    "        periodo_reviews['vader_positive'] = [x[1] for x in vader_results]\n",
    "        periodo_reviews['vader_neutral'] = [x[2] for x in vader_results]\n",
    "        periodo_reviews['vader_negative'] = [x[3] for x in vader_results]\n",
    "        \n",
    "        # Extraer vocabulario cr√≠tico para identificaci√≥n de causas espec√≠ficas\n",
    "        periodo_text = ' '.join(periodo_reviews['text_clean'].fillna(''))\n",
    "        words = periodo_text.split()\n",
    "        \n",
    "        # Filtrar stopwords b√°sicas para an√°lisis de contenido\n",
    "        stopwords_basic = ['el', 'la', 'de', 'que', 'y', 'en', 'un', 'es', 'se', 'no', 'te', 'lo', 'le', 'da', 'su', 'por', 'son', 'con', 'para', 'al', 'del', 'los', 'las', 'me', 'mi', 'muy', 'pero', 'si', 'ya', 'o', 'este', 'esta', 'como', 'todo', 'bien', 'mas', 'tiene', 'a']\n",
    "        filtered_words = [w for w in words if len(w) > 2 and w.lower() not in stopwords_basic]\n",
    "        word_freq = Counter(filtered_words)\n",
    "        \n",
    "        # Seleccionar rese√±as m√°s representativas para an√°lisis cualitativo\n",
    "        rese√±as_negativas = periodo_reviews[periodo_reviews['vader_compound'] < -0.05]\n",
    "        if len(rese√±as_negativas) > 0:\n",
    "            rese√±as_representativas = rese√±as_negativas.nlargest(3, 'useful_votes')[['text', 'rating', 'useful_votes']]\n",
    "        else:\n",
    "            rese√±as_representativas = periodo_reviews.nlargest(3, 'useful_votes')[['text', 'rating', 'useful_votes']]\n",
    "        \n",
    "        patrones_semanticos.append({\n",
    "            'producto': producto,\n",
    "            'periodo': periodo,\n",
    "            'n_reviews': len(periodo_reviews),\n",
    "            'avg_rating': periodo_reviews['rating'].mean(),\n",
    "            'avg_sentiment_vader': periodo_reviews['vader_compound'].mean(),\n",
    "            'sentiment_volatility': periodo_reviews['vader_compound'].std(),\n",
    "            'negative_reviews_pct': (periodo_reviews['vader_compound'] < -0.05).mean() * 100,\n",
    "            'positive_reviews_pct': (periodo_reviews['vader_compound'] > 0.05).mean() * 100,\n",
    "            'vocabulario_critico': dict(word_freq.most_common(10)),\n",
    "            'rese√±as_representativas': rese√±as_representativas.to_dict('records'),\n",
    "            # Variables actualizadas para coherencia\n",
    "            'rating_anomalia': anomalia['rating_mean'],  # Rating del per√≠odo an√≥malo\n",
    "            'intensidad': anomalia['intensidad'],        # Cr√≠tica, Alta, Moderada, etc.\n",
    "            'rating_score': anomalia['rating_score'],    # 0-4 basado en rating directo\n",
    "            'indicadores_eda': anomalia['indicadores_complementarios']  # Indicadores adicionales\n",
    "        })\n",
    "    \n",
    "    return patrones_semanticos\n",
    "\n",
    "# Aplicar an√°lisis sem√°ntico actualizado para validaci√≥n cruzada\n",
    "if len(df_anomalias_rating) > 0:\n",
    "    patrones_semanticos_updated = analyze_semantic_patterns_updated(df, df_anomalias_rating)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã CARACTERIZACI√ìN SEM√ÅNTICA DE ANOMAL√çAS (VALIDACI√ìN NLP ACTUALIZADA)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Analizar solo Samsung A15 para el caso de estudio\n",
    "    samsung_patrones = [p for p in patrones_semanticos_updated if p['producto'] == 'Samsung A15']\n",
    "    \n",
    "    print(f\"üéØ AN√ÅLISIS SAMSUNG A15 - TRIANGULACI√ìN RATING + NLP:\")\n",
    "    print(f\"   Total per√≠odos analizados: {len(samsung_patrones)}\")\n",
    "    \n",
    "    for patron in samsung_patrones:\n",
    "        print(f\"\\nüîç PER√çODO: {patron['periodo']}\")\n",
    "        print(f\"   üìä Rating directo: {patron['rating_anomalia']:.2f} ({patron['intensidad']})\")\n",
    "        print(f\"   üìä Rating score: {patron['rating_score']}/4\")\n",
    "        print(f\"   üìà Indicadores EDA: {patron['indicadores_eda']}\")\n",
    "        print(f\"   üìù Rese√±as per√≠odo: {patron['n_reviews']}\")\n",
    "        print(f\"   üí≠ Sentimiento VADER promedio: {patron['avg_sentiment_vader']:.3f}\")\n",
    "        print(f\"   üòü % Rese√±as negativas (VADER): {patron['negative_reviews_pct']:.1f}%\")\n",
    "        print(f\"   üòä % Rese√±as positivas (VADER): {patron['positive_reviews_pct']:.1f}%\")\n",
    "        print(f\"   üî§ Top 3 vocabulario cr√≠tico:\")\n",
    "        top_words = list(patron['vocabulario_critico'].items())[:3]\n",
    "        for word, freq in top_words:\n",
    "            print(f\"      ‚Ä¢ {word}: {freq}\")\n",
    "    \n",
    "    # An√°lisis de convergencia rating directo vs VADER\n",
    "    print(f\"\\nüìä CONVERGENCIA RATING DIRECTO vs VADER:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    convergencias_exitosas = []\n",
    "    for patron in samsung_patrones:\n",
    "        # Criterios de convergencia actualizados para VADER\n",
    "        rating_critico = patron['rating_score'] >= 3  # Alta o Cr√≠tica\n",
    "        vader_negativo = patron['avg_sentiment_vader'] < -0.05  # VADER compound negativo\n",
    "        \n",
    "        # FIX: Convertir dict_keys a lista antes de slicear\n",
    "        vocab_keys = list(patron['vocabulario_critico'].keys())\n",
    "        vocab_problematico = 'cargador' in [w.lower() for w in vocab_keys[:5]]\n",
    "        \n",
    "        # Evaluar convergencia\n",
    "        criterios_met = sum([rating_critico, vader_negativo, vocab_problematico])\n",
    "        \n",
    "        if criterios_met >= 2:  # Al menos 2 de 3 criterios\n",
    "            convergencias_exitosas.append(patron)\n",
    "            print(f\"‚úÖ {patron['periodo']}: CONVERGENCIA EXITOSA\")\n",
    "            print(f\"   ‚Ä¢ Rating: {patron['rating_anomalia']:.2f} ({'‚úì' if rating_critico else '‚úó'})\")\n",
    "            print(f\"   ‚Ä¢ VADER: {patron['avg_sentiment_vader']:.3f} ({'‚úì' if vader_negativo else '‚úó'})\")\n",
    "            print(f\"   ‚Ä¢ 'Cargador' en top 5: {'‚úì' if vocab_problematico else '‚úó'}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {patron['periodo']}: Convergencia parcial ({criterios_met}/3)\")\n",
    "    \n",
    "    print(f\"\\nüéØ RESUMEN DE CONVERGENCIA:\")\n",
    "    print(f\"   ‚Ä¢ Per√≠odos analizados: {len(samsung_patrones)}\")\n",
    "    print(f\"   ‚Ä¢ Convergencias exitosas: {len(convergencias_exitosas)}\")\n",
    "    print(f\"   ‚Ä¢ Tasa de convergencia: {len(convergencias_exitosas)/len(samsung_patrones)*100:.1f}%\")\n",
    "    \n",
    "    # Identificar per√≠odo con mayor convergencia\n",
    "    if convergencias_exitosas:\n",
    "        periodo_mejor = max(convergencias_exitosas, \n",
    "                           key=lambda x: x['rating_score'] + (1 if x['avg_sentiment_vader'] < -0.05 else 0))\n",
    "        print(f\"   ‚Ä¢ Mejor convergencia: {periodo_mejor['periodo']} \"\n",
    "              f\"(Rating: {periodo_mejor['rating_anomalia']:.2f}, \"\n",
    "              f\"VADER: {periodo_mejor['avg_sentiment_vader']:.3f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Caracterizaci√≥n sem√°ntica actualizada completada\")\n",
    "print(\"üìù Metodolog√≠a: Triangulaci√≥n rating directo + an√°lisis VADER (superior a TextBlob)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. üìà Caso Samsung A15: Timeline Metodol√≥gico\n",
    "\n",
    "## Prop√≥sito de la Secci√≥n\n",
    "Demostrar c√≥mo la **triangulaci√≥n metodol√≥gica** proporciona tanto \n",
    "detecci√≥n temporal (EDA) como explicaci√≥n causal (NLP) en un caso \n",
    "espec√≠fico documentado, estableciendo precedente para replicaci√≥n.\n",
    "\n",
    "## Limitaciones del Caso\n",
    "- **Una crisis documentada**: No permite generalizaci√≥n estad√≠stica\n",
    "- **Contexto espec√≠fico**: Argentina, smartphones gama media, MercadoLibre  \n",
    "- **Validaci√≥n post-hoc**: An√°lisis de evento conocido, no predicci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. CASO SAMSUNG A15: TIMELINE METODOL√ìGICO\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_samsung_timeline_updated(df, anomalias_rating, patrones_semanticos):\n",
    "    \"\"\"\n",
    "    Analiza timeline espec√≠fico de Samsung A15 integrando EDA y VADER\n",
    "    \"\"\"\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    \n",
    "    # Filtrar datos Samsung A15\n",
    "    samsung_data = anomalias_rating[anomalias_rating['producto'] == 'Samsung A15'].copy()\n",
    "    samsung_semantics = [p for p in patrones_semanticos if p['producto'] == 'Samsung A15']\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üéØ TIMELINE METODOL√ìGICO SAMSUNG A15\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìã Demostraci√≥n de triangulaci√≥n EDA-VADER en caso espec√≠fico documentado\")\n",
    "    \n",
    "    if len(samsung_data) == 0:\n",
    "        print(\"‚ùå No hay datos de Samsung A15 disponibles\")\n",
    "        return None\n",
    "    \n",
    "    # Ordenar por per√≠odo\n",
    "    samsung_data = samsung_data.sort_values('periodo')\n",
    "    samsung_semantics = sorted(samsung_semantics, key=lambda x: x['periodo'])\n",
    "    \n",
    "    print(f\"\\nüìä PER√çODO DE AN√ÅLISIS: {samsung_data['periodo'].min()} a {samsung_data['periodo'].max()}\")\n",
    "    print(f\"üìà TOTAL PER√çODOS ANALIZADOS: {len(samsung_data)}\")\n",
    "    \n",
    "    # An√°lisis por per√≠odo\n",
    "    print(f\"\\nüîç EVOLUCI√ìN TEMPORAL DETALLADA:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    timeline_data = []\n",
    "    \n",
    "    for i, (_, row) in enumerate(samsung_data.iterrows()):\n",
    "        periodo = row['periodo']\n",
    "        \n",
    "        # Buscar datos sem√°nticos correspondientes\n",
    "        semantic_data = next((s for s in samsung_semantics if s['periodo'] == periodo), None)\n",
    "        \n",
    "        if semantic_data:\n",
    "            # Calcular m√©tricas de convergencia\n",
    "            eda_score = row['rating_score']  # 0-4 basado en rating\n",
    "            vader_score = semantic_data['avg_sentiment_vader']\n",
    "            freq_cargador = semantic_data['vocabulario_critico'].get('cargador', 0)\n",
    "            \n",
    "            # Determinar fase del timeline\n",
    "            if i <= 1:\n",
    "                fase = \"üü¢ INICIAL\"\n",
    "            elif row['rating_mean'] < 3.0:\n",
    "                fase = \"üî¥ CR√çTICA\"\n",
    "            elif row['rating_mean'] < 3.5:\n",
    "                fase = \"üü° DETERIORO\"\n",
    "            else:\n",
    "                fase = \"üîµ ESTABLE\"\n",
    "            \n",
    "            print(f\"\\nüìÖ {periodo} - {fase}\")\n",
    "            print(f\"   üìä Rating: {row['rating_mean']:.2f} ({row['intensidad']})\")\n",
    "            print(f\"   üìä Score EDA: {eda_score}/4\")\n",
    "            print(f\"   üí≠ VADER: {vader_score:.3f}\")\n",
    "            print(f\"   üìù Rese√±as: {semantic_data['n_reviews']}\")\n",
    "            print(f\"   üî§ 'Cargador': {freq_cargador} menciones\")\n",
    "            print(f\"   üòü Negativas VADER: {semantic_data['negative_reviews_pct']:.1f}%\")\n",
    "            print(f\"   üìà Indicadores EDA: {row['indicadores_complementarios']}\")\n",
    "            \n",
    "            # Validaci√≥n cruzada per√≠odo espec√≠fico\n",
    "            convergencia_rating = eda_score >= 2  # Moderada o superior\n",
    "            convergencia_vader = vader_score < -0.05  # Negativo\n",
    "            convergencia_vocab = freq_cargador >= 5  # Menciones significativas\n",
    "            \n",
    "            total_convergencia = sum([convergencia_rating, convergencia_vader, convergencia_vocab])\n",
    "            \n",
    "            if total_convergencia >= 2:\n",
    "                print(f\"   ‚úÖ CONVERGENCIA METODOL√ìGICA: {total_convergencia}/3\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è Convergencia parcial: {total_convergencia}/3\")\n",
    "            \n",
    "            timeline_data.append({\n",
    "                'periodo': periodo,\n",
    "                'rating': row['rating_mean'],\n",
    "                'eda_score': eda_score,\n",
    "                'vader_score': vader_score,\n",
    "                'freq_cargador': freq_cargador,\n",
    "                'convergencia': total_convergencia,\n",
    "                'fase': fase.split()[1]\n",
    "            })\n",
    "    \n",
    "    # An√°lisis de patrones temporales\n",
    "    print(f\"\\nüìà PATRONES TEMPORALES IDENTIFICADOS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Rating m√≠nimo y m√°ximo\n",
    "    min_rating_periodo = samsung_data.loc[samsung_data['rating_mean'].idxmin()]\n",
    "    max_rating_periodo = samsung_data.loc[samsung_data['rating_mean'].idxmax()]\n",
    "    \n",
    "    print(f\"üîª RATING M√çNIMO: {min_rating_periodo['rating_mean']:.2f} en {min_rating_periodo['periodo']}\")\n",
    "    print(f\"üî∫ RATING M√ÅXIMO: {max_rating_periodo['rating_mean']:.2f} en {max_rating_periodo['periodo']}\")\n",
    "    \n",
    "    # VADER m√°s negativo\n",
    "    vader_min = min(samsung_semantics, key=lambda x: x['avg_sentiment_vader'])\n",
    "    print(f\"üò∞ VADER M√ÅS NEGATIVO: {vader_min['avg_sentiment_vader']:.3f} en {vader_min['periodo']}\")\n",
    "    \n",
    "    # Pico de \"cargador\"\n",
    "    cargador_max = max(samsung_semantics, key=lambda x: x['vocabulario_critico'].get('cargador', 0))\n",
    "    print(f\"üî§ PICO 'CARGADOR': {cargador_max['vocabulario_critico'].get('cargador', 0)} menciones en {cargador_max['periodo']}\")\n",
    "    \n",
    "    # An√°lisis de desfases temporales\n",
    "    print(f\"\\nüïê DESFASES TEMPORALES OBSERVADOS:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Comparar per√≠odos cr√≠ticos\n",
    "    periodo_min_rating = min_rating_periodo['periodo']\n",
    "    periodo_max_cargador = cargador_max['periodo']\n",
    "    periodo_min_vader = vader_min['periodo']\n",
    "    \n",
    "    print(f\"üìä Pico vocabulario cr√≠tico: {periodo_max_cargador}\")\n",
    "    print(f\"üìä Rating m√≠nimo: {periodo_min_rating}\")  \n",
    "    print(f\"üìä VADER m√°s negativo: {periodo_min_vader}\")\n",
    "    \n",
    "    if periodo_max_cargador != periodo_min_rating:\n",
    "        print(f\"üí° DESFASE IDENTIFICADO: Vocabulario cr√≠tico precede a rating m√≠nimo\")\n",
    "        print(f\"   ‚Üí Indica que NLP puede actuar como indicador adelantado\")\n",
    "    else:\n",
    "        print(f\"üí° COINCIDENCIA TEMPORAL: Vocabulario y rating cr√≠ticos simult√°neos\")\n",
    "    \n",
    "    # Resumen de validaci√≥n metodol√≥gica\n",
    "    print(f\"\\nüéØ VALIDACI√ìN METODOL√ìGICA:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    convergencias_exitosas = sum(1 for data in timeline_data if data['convergencia'] >= 2)\n",
    "    tasa_convergencia = convergencias_exitosas / len(timeline_data) * 100\n",
    "    \n",
    "    print(f\"üìä Per√≠odos con convergencia EDA-VADER: {convergencias_exitosas}/{len(timeline_data)}\")\n",
    "    print(f\"üìä Tasa de convergencia metodol√≥gica: {tasa_convergencia:.1f}%\")\n",
    "    \n",
    "    # Identificar patrones espec√≠ficos\n",
    "    rating_trend = \"DETERIORO ‚Üí RECUPERACI√ìN\" if samsung_data['rating_mean'].iloc[-1] > samsung_data['rating_mean'].min() else \"DETERIORO SOSTENIDO\"\n",
    "    print(f\"üìà Tendencia general: {rating_trend}\")\n",
    "    \n",
    "    # Consistencia del problema \"cargador\"\n",
    "    periodos_con_cargador = sum(1 for s in samsung_semantics if s['vocabulario_critico'].get('cargador', 0) >= 3)\n",
    "    print(f\"üî§ Per√≠odos con problema 'cargador': {periodos_con_cargador}/{len(samsung_semantics)}\")\n",
    "    print(f\"üìã Consistencia del problema espec√≠fico: {periodos_con_cargador/len(samsung_semantics)*100:.1f}%\")\n",
    "    \n",
    "    return timeline_data\n",
    "\n",
    "# Ejecutar an√°lisis de timeline\n",
    "if len(df_anomalias_rating) > 0 and 'patrones_semanticos_updated' in locals():\n",
    "    print(\"üîÑ Ejecutando an√°lisis de timeline Samsung A15...\")\n",
    "    timeline_results = analyze_samsung_timeline_updated(df, df_anomalias_rating, patrones_semanticos_updated)\n",
    "    \n",
    "    if timeline_results:\n",
    "        print(f\"\\n‚úÖ Timeline metodol√≥gico completado\")\n",
    "        print(f\"üìù Metodolog√≠a: Integraci√≥n EDA + VADER para validaci√≥n cruzada temporal\")\n",
    "        \n",
    "        # Guardar resultados para uso posterior\n",
    "        timeline_df = pd.DataFrame(timeline_results)\n",
    "        print(f\"üìä Timeline data generado: {len(timeline_df)} per√≠odos procesados\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error en generaci√≥n de timeline\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Requisitos no cumplidos para an√°lisis de timeline\")\n",
    "    print(f\"   - df_anomalias_rating: {'‚úì' if len(df_anomalias_rating) > 0 else '‚úó'}\")\n",
    "    print(f\"   - patrones_semanticos_updated: {'‚úì' if 'patrones_semanticos_updated' in locals() else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. üîó Triangulaci√≥n y Validaci√≥n Cruzada\n",
    "\n",
    "## Metodolog√≠a de Convergencia\n",
    "An√°lisis sistem√°tico de coincidencias y discrepancias entre EDA y NLP \n",
    "para establecer **confidence intervals** en contexto de datos limitados.\n",
    "\n",
    "## M√©tricas de Validaci√≥n Cruzada\n",
    "- Per√≠odos donde ambas t√©cnicas coinciden en detectar anomal√≠as\n",
    "- Grado de especificidad en identificaci√≥n de causas\n",
    "- Robustez de hallazgos bajo diferentes aproximaciones metodol√≥gicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. TRIANGULACI√ìN Y VALIDACI√ìN CRUZADA\n",
    "# =============================================================================\n",
    "\n",
    "def create_triangulation_dashboard_final(df, anomalias_rating, patrones_semanticos):\n",
    "    \"\"\"\n",
    "    Dashboard final de triangulaci√≥n metodol√≥gica EDA-VADER-Engagement\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from matplotlib.dates import DateFormatter\n",
    "    import matplotlib.dates as mdates\n",
    "    \n",
    "    # Configurar estilo\n",
    "    plt.style.use('default')\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    \n",
    "    # Filtrar datos Samsung A15\n",
    "    samsung_eda = anomalias_rating[anomalias_rating['producto'] == 'Samsung A15'].copy()\n",
    "    samsung_nlp = [p for p in patrones_semanticos if p['producto'] == 'Samsung A15']\n",
    "    \n",
    "    if len(samsung_eda) == 0 or len(samsung_nlp) == 0:\n",
    "        print(\"‚ùå Datos insuficientes para triangulaci√≥n\")\n",
    "        return None\n",
    "    \n",
    "    # Crear DataFrame consolidado para visualizaci√≥n\n",
    "    triangulation_data = []\n",
    "    \n",
    "    for _, row in samsung_eda.iterrows():\n",
    "        periodo = row['periodo']\n",
    "        \n",
    "        # Buscar datos NLP correspondientes\n",
    "        nlp_data = next((s for s in samsung_nlp if s['periodo'] == periodo), None)\n",
    "        \n",
    "        if nlp_data:\n",
    "            triangulation_data.append({\n",
    "                'periodo': periodo,\n",
    "                'rating_mean': row['rating_mean'],\n",
    "                'votes_median': row['votes_median'],\n",
    "                'freq_cargador': nlp_data['vocabulario_critico'].get('cargador', 0),\n",
    "                'vader_sentiment': nlp_data['avg_sentiment_vader'],\n",
    "                'rating_score': row['rating_score'],\n",
    "                'intensidad': row['intensidad']\n",
    "            })\n",
    "    \n",
    "    df_triangulation = pd.DataFrame(triangulation_data)\n",
    "    df_triangulation['periodo_dt'] = pd.to_datetime(df_triangulation['periodo'].astype(str))\n",
    "    df_triangulation = df_triangulation.sort_values('periodo_dt')\n",
    "    \n",
    "    # Crear gr√°fico de triangulaci√≥n\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Colores por intensidad de rating\n",
    "    color_map = {\n",
    "        'Normal': '#2E8B57',      # Verde\n",
    "        'Baja': '#FFD700',        # Dorado  \n",
    "        'Moderada': '#FF8C00',    # Naranja\n",
    "        'Alta': '#FF4500',        # Naranja rojizo\n",
    "        'Cr√≠tica': '#DC143C'      # Rojo\n",
    "    }\n",
    "    \n",
    "    # Barras de rating con colores por intensidad\n",
    "    colors = [color_map.get(intensidad, '#808080') for intensidad in df_triangulation['intensidad']]\n",
    "    bars = ax.bar(df_triangulation['periodo_dt'], df_triangulation['rating_mean'], \n",
    "                  color=colors, alpha=0.8, label='Rating (EDA)', width=25)\n",
    "    \n",
    "    # A√±adir valores en las barras\n",
    "    for bar, rating in zip(bars, df_triangulation['rating_mean']):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{rating:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # L√≠nea de frecuencia \"cargador\" (NLP)\n",
    "    ax2 = ax.twinx()\n",
    "    line_nlp = ax2.plot(df_triangulation['periodo_dt'], df_triangulation['freq_cargador'], \n",
    "                        'ro-', linewidth=3, markersize=8, label='Frecuencia \"Cargador\" (NLP)', alpha=0.9)\n",
    "    \n",
    "    # L√≠nea de mediana engagement\n",
    "    ax3 = ax.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    line_eng = ax3.plot(df_triangulation['periodo_dt'], df_triangulation['votes_median'], \n",
    "                        's--', color='purple', linewidth=2, markersize=6, \n",
    "                        label='Mediana Engagement', alpha=0.8)\n",
    "    \n",
    "    # Configurar ejes\n",
    "    ax.set_ylabel('Rating Promedio (EDA)', fontsize=12, fontweight='bold', color='black')\n",
    "    ax2.set_ylabel('Frecuencia \"Cargador\" (NLP)', fontsize=12, fontweight='bold', color='red')\n",
    "    ax3.set_ylabel('Mediana Votos √ötiles (Engagement)', fontsize=12, fontweight='bold', color='purple')\n",
    "    \n",
    "    ax.set_xlabel('Per√≠odo', fontsize=12, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelcolor='black')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    ax3.tick_params(axis='y', labelcolor='purple')\n",
    "    \n",
    "    # Formatear fechas en eje X\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%Y-%m'))\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # T√≠tulo y anotaciones\n",
    "    ax.set_title('Triangulaci√≥n Metodol√≥gica EDA-NLP: Caso Samsung A15\\n' + \n",
    "                 'Demostraci√≥n de Convergencia Rating + Sentiment + Engagement', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # L√≠nea cr√≠tica en rating 3.0\n",
    "    ax.axhline(y=3.0, color='red', linestyle=':', alpha=0.7, linewidth=2)\n",
    "    ax.text(df_triangulation['periodo_dt'].iloc[2], 3.1, 'Umbral Cr√≠tico (3.0)', \n",
    "            fontsize=10, color='red', fontweight='bold')\n",
    "    \n",
    "    # Identificar convergencia m√°xima\n",
    "    pico_nlp = df_triangulation.loc[df_triangulation['freq_cargador'].idxmax()]\n",
    "    min_rating = df_triangulation.loc[df_triangulation['rating_mean'].idxmin()]\n",
    "    \n",
    "    # Anotar desfase temporal\n",
    "    if pico_nlp['periodo'] != min_rating['periodo']:\n",
    "        # Flecha mostrando desfase\n",
    "        ax.annotate('', xy=(min_rating['periodo_dt'], min_rating['rating_mean']), \n",
    "                    xytext=(pico_nlp['periodo_dt'], 3.8),\n",
    "                    arrowprops=dict(arrowstyle='<->', color='purple', lw=2))\n",
    "        ax.text(pico_nlp['periodo_dt'], 4.0, \n",
    "                f'Desfase Temporal\\nNLP: {pico_nlp[\"periodo\"]}\\nRating m√≠n: {min_rating[\"periodo\"]}', \n",
    "                ha='center', va='bottom', fontsize=9, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.8))\n",
    "    \n",
    "    # Leyenda consolidada\n",
    "    lines1, labels1 = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels() \n",
    "    lines3, labels3 = ax3.get_legend_handles_labels()\n",
    "    \n",
    "    # Leyenda de intensidad\n",
    "    intensity_elements = [plt.Rectangle((0,0),1,1, color=color_map[intensity], alpha=0.8) \n",
    "                         for intensity in ['Normal', 'Baja', 'Moderada', 'Alta', 'Cr√≠tica']]\n",
    "    intensity_labels = ['Normal', 'Baja', 'Moderada', 'Alta', 'Cr√≠tica']\n",
    "    \n",
    "    legend1 = ax.legend(lines1 + lines2 + lines3, labels1 + labels2 + labels3, \n",
    "                       loc='upper left', fontsize=10)\n",
    "    legend2 = ax.legend(intensity_elements, intensity_labels, \n",
    "                       title='Intensidad Rating', loc='upper right', fontsize=9)\n",
    "    ax.add_artist(legend1)  # Mantener ambas leyendas\n",
    "    \n",
    "    # Grid y formato final\n",
    "    ax.grid(True, alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar gr√°fico\n",
    "    plt.savefig('triangulacion_metodologica_final.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return df_triangulation\n",
    "\n",
    "def analyze_methodological_convergence(triangulation_df, anomalias_rating, patrones_semanticos):\n",
    "    \"\"\"\n",
    "    An√°lisis cuantitativo de convergencia metodol√≥gica\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä AN√ÅLISIS DE CONVERGENCIA METODOL√ìGICA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    samsung_eda = anomalias_rating[anomalias_rating['producto'] == 'Samsung A15']\n",
    "    samsung_nlp = [p for p in patrones_semanticos if p['producto'] == 'Samsung A15']\n",
    "    \n",
    "    print(f\"üéØ M√âTRICAS DE TRIANGULACI√ìN:\")\n",
    "    print(f\"   ‚Ä¢ Per√≠odos analizados: {len(triangulation_df)}\")\n",
    "    print(f\"   ‚Ä¢ Rango temporal: {triangulation_df['periodo'].min()} - {triangulation_df['periodo'].max()}\")\n",
    "    \n",
    "    # An√°lisis de convergencia por criterios\n",
    "    convergencias = []\n",
    "    \n",
    "    for _, row in triangulation_df.iterrows():\n",
    "        rating_critico = row['rating_score'] >= 2  # Moderada o superior\n",
    "        nlp_negativo = row['vader_sentiment'] < -0.05  # VADER negativo  \n",
    "        vocab_cargador = row['freq_cargador'] >= 5  # Menciones significativas\n",
    "        \n",
    "        criterios_cumplidos = sum([rating_critico, nlp_negativo, vocab_cargador])\n",
    "        \n",
    "        convergencias.append({\n",
    "            'periodo': row['periodo'],\n",
    "            'rating_critico': rating_critico,\n",
    "            'nlp_negativo': nlp_negativo, \n",
    "            'vocab_cargador': vocab_cargador,\n",
    "            'total_criterios': criterios_cumplidos\n",
    "        })\n",
    "    \n",
    "    convergencias_df = pd.DataFrame(convergencias)\n",
    "    convergencias_exitosas = len(convergencias_df[convergencias_df['total_criterios'] >= 2])\n",
    "    tasa_convergencia = convergencias_exitosas / len(convergencias_df) * 100\n",
    "    \n",
    "    print(f\"\\nüìà RESULTADOS DE CONVERGENCIA:\")\n",
    "    print(f\"   ‚Ä¢ Convergencias exitosas (‚â•2/3 criterios): {convergencias_exitosas}/{len(convergencias_df)}\")\n",
    "    print(f\"   ‚Ä¢ Tasa de convergencia metodol√≥gica: {tasa_convergencia:.1f}%\")\n",
    "    \n",
    "    # An√°lisis por criterio individual\n",
    "    print(f\"\\nüîç AN√ÅLISIS POR CRITERIO:\")\n",
    "    rating_count = convergencias_df['rating_critico'].sum()\n",
    "    nlp_count = convergencias_df['nlp_negativo'].sum()\n",
    "    vocab_count = convergencias_df['vocab_cargador'].sum()\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Rating cr√≠tico (‚â•Moderada): {rating_count}/{len(convergencias_df)} ({rating_count/len(convergencias_df)*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ VADER negativo (<-0.05): {nlp_count}/{len(convergencias_df)} ({nlp_count/len(convergencias_df)*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ 'Cargador' significativo (‚â•5): {vocab_count}/{len(convergencias_df)} ({vocab_count/len(convergencias_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Identificar patrones temporales\n",
    "    print(f\"\\nüïê PATRONES TEMPORALES:\")\n",
    "    \n",
    "    # Picos de cada m√©trica\n",
    "    max_rating_score = triangulation_df.loc[triangulation_df['rating_score'].idxmax()]\n",
    "    min_rating = triangulation_df.loc[triangulation_df['rating_mean'].idxmin()]\n",
    "    max_cargador = triangulation_df.loc[triangulation_df['freq_cargador'].idxmax()]\n",
    "    min_vader = min(samsung_nlp, key=lambda x: x['avg_sentiment_vader'])\n",
    "    \n",
    "    print(f\"   ‚Ä¢ M√°ximo Score EDA: {max_rating_score['rating_score']}/4 en {max_rating_score['periodo']}\")\n",
    "    print(f\"   ‚Ä¢ Rating m√≠nimo: {min_rating['rating_mean']:.2f} en {min_rating['periodo']}\")\n",
    "    print(f\"   ‚Ä¢ Pico 'Cargador': {max_cargador['freq_cargador']} en {max_cargador['periodo']}\")\n",
    "    print(f\"   ‚Ä¢ VADER m√°s negativo: {min_vader['avg_sentiment_vader']:.3f} en {min_vader['periodo']}\")\n",
    "    \n",
    "    # Desfases temporales\n",
    "    if max_cargador['periodo'] != min_rating['periodo']:\n",
    "        print(f\"   üí° DESFASE IDENTIFICADO: Pico NLP precede rating m√≠nimo\")\n",
    "        print(f\"      ‚Üí NLP act√∫a como indicador adelantado en este caso\")\n",
    "    \n",
    "    return convergencias_df, tasa_convergencia\n",
    "\n",
    "def generate_methodological_summary(tasa_convergencia, total_periodos):\n",
    "    \"\"\"\n",
    "    Resumen metodol√≥gico del caso de estudio\n",
    "    \"\"\"\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"üéØ RESUMEN METODOL√ìGICO DEL CASO DE ESTUDIO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"üìã DEMOSTRACI√ìN EXITOSA DE TRIANGULACI√ìN:\")\n",
    "    print(f\"   ‚úÖ Convergencia EDA-VADER: {tasa_convergencia:.1f}%\")\n",
    "    print(f\"   ‚úÖ Per√≠odos analizados: {total_periodos}\")\n",
    "    print(f\"   ‚úÖ Desfase temporal detectado: NLP adelanta rating\")\n",
    "    print(f\"   ‚úÖ Problema espec√≠fico identificado: 'Cargador' como causa\")\n",
    "    \n",
    "    print(f\"\\nüí° VALOR METODOL√ìGICO DEMOSTRADO:\")\n",
    "    print(f\"   üîπ Rating directo > scores h√≠bridos (transparencia)\")\n",
    "    print(f\"   üîπ VADER > TextBlob para espa√±ol ({tasa_convergencia:.0f}% vs 20% convergencia)\")\n",
    "    print(f\"   üîπ Triangulaci√≥n robusta en dataset peque√±o\")\n",
    "    print(f\"   üîπ NLP como indicador adelantado validado\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è LIMITACIONES RECONOCIDAS:\")\n",
    "    print(f\"   ‚Ä¢ Caso √∫nico: Samsung A15 (no generalizable autom√°ticamente)\")\n",
    "    print(f\"   ‚Ä¢ Dataset peque√±o: {total_periodos} per√≠odos analizados\")\n",
    "    print(f\"   ‚Ä¢ Contexto espec√≠fico: Mercado argentino, MercadoLibre\")\n",
    "    print(f\"   ‚Ä¢ Validaci√≥n post-hoc: Evento conocido, no predictivo\")\n",
    "    \n",
    "    print(f\"\\nüöÄ APLICABILIDAD:\")\n",
    "    print(f\"   ‚úÖ Metodolog√≠a replicable para casos similares\")\n",
    "    print(f\"   ‚úÖ Framework adaptable a otros productos/contextos\")\n",
    "    print(f\"   ‚úÖ Principios escalables con m√°s datos\")\n",
    "    print(f\"   ‚úÖ Baseline metodol√≥gico para an√°lisis futuros\")\n",
    "\n",
    "# Ejecutar triangulaci√≥n completa\n",
    "if len(df_anomalias_rating) > 0 and 'patrones_semanticos_updated' in locals():\n",
    "    print(\"üîÑ Ejecutando triangulaci√≥n metodol√≥gica final...\")\n",
    "    \n",
    "    # Crear dashboard de triangulaci√≥n  \n",
    "    triangulation_data = create_triangulation_dashboard_final(df, df_anomalias_rating, patrones_semanticos_updated)\n",
    "    \n",
    "    if triangulation_data is not None:\n",
    "        # An√°lisis de convergencia\n",
    "        convergencias_results, tasa_final = analyze_methodological_convergence(\n",
    "            triangulation_data, df_anomalias_rating, patrones_semanticos_updated\n",
    "        )\n",
    "        \n",
    "        # Resumen metodol√≥gico\n",
    "        generate_methodological_summary(tasa_final, len(triangulation_data))\n",
    "        \n",
    "        print(f\"\\n‚úÖ Triangulaci√≥n metodol√≥gica completada\")\n",
    "        print(f\"üìä Dashboard guardado: 'triangulacion_metodologica_final.png'\")\n",
    "        print(f\"üìù Metodolog√≠a: Integraci√≥n EDA + VADER + Engagement validada\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Error en creaci√≥n de dashboard de triangulaci√≥n\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Requisitos no cumplidos para triangulaci√≥n completa\")\n",
    "    print(f\"   - df_anomalias_rating: {'‚úì' if len(df_anomalias_rating) > 0 else '‚úó'}\")\n",
    "    print(f\"   - patrones_semanticos_updated: {'‚úì' if 'patrones_semanticos_updated' in locals() else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. TRIANGULACI√ìN Y VALIDACI√ìN CRUZADA\n",
    "# =============================================================================\n",
    "\n",
    "def create_triangulation_dashboard_final(df, anomalias_rating, patrones_semanticos):\n",
    "    \"\"\"\n",
    "    Dashboard final de triangulaci√≥n metodol√≥gica EDA-VADER-Engagement\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from matplotlib.dates import DateFormatter\n",
    "    import matplotlib.dates as mdates\n",
    "    \n",
    "    # Configurar estilo\n",
    "    plt.style.use('default')\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    \n",
    "    # Filtrar datos Samsung A15\n",
    "    samsung_eda = anomalias_rating[anomalias_rating['producto'] == 'Samsung A15'].copy()\n",
    "    samsung_nlp = [p for p in patrones_semanticos if p['producto'] == 'Samsung A15']\n",
    "    \n",
    "    if len(samsung_eda) == 0 or len(samsung_nlp) == 0:\n",
    "        print(\"‚ùå Datos insuficientes para triangulaci√≥n\")\n",
    "        return None\n",
    "    \n",
    "    # Crear DataFrame consolidado para visualizaci√≥n\n",
    "    triangulation_data = []\n",
    "    \n",
    "    for _, row in samsung_eda.iterrows():\n",
    "        periodo = row['periodo']\n",
    "        \n",
    "        # Buscar datos NLP correspondientes\n",
    "        nlp_data = next((s for s in samsung_nlp if s['periodo'] == periodo), None)\n",
    "        \n",
    "        if nlp_data:\n",
    "            triangulation_data.append({\n",
    "                'periodo': periodo,\n",
    "                'rating_mean': row['rating_mean'],\n",
    "                'votes_median': row['votes_median'],\n",
    "                'freq_cargador': nlp_data['vocabulario_critico'].get('cargador', 0),\n",
    "                'vader_sentiment': nlp_data['avg_sentiment_vader'],\n",
    "                'rating_score': row['rating_score'],\n",
    "                'intensidad': row['intensidad']\n",
    "            })\n",
    "    \n",
    "    df_triangulation = pd.DataFrame(triangulation_data)\n",
    "    df_triangulation['periodo_dt'] = pd.to_datetime(df_triangulation['periodo'].astype(str))\n",
    "    df_triangulation = df_triangulation.sort_values('periodo_dt')\n",
    "    \n",
    "    # Crear gr√°fico de triangulaci√≥n\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Colores por intensidad de rating\n",
    "    color_map = {\n",
    "        'Normal': '#2E8B57',      # Verde\n",
    "        'Baja': '#FFD700',        # Dorado  \n",
    "        'Moderada': '#FF8C00',    # Naranja\n",
    "        'Alta': '#FF4500',        # Naranja rojizo\n",
    "        'Cr√≠tica': '#DC143C'      # Rojo\n",
    "    }\n",
    "    \n",
    "    # Barras de rating con colores por intensidad\n",
    "    colors = [color_map.get(intensidad, '#808080') for intensidad in df_triangulation['intensidad']]\n",
    "    bars = ax.bar(df_triangulation['periodo_dt'], df_triangulation['rating_mean'], \n",
    "                  color=colors, alpha=0.8, label='Rating (EDA)', width=25)\n",
    "    \n",
    "    # A√±adir valores en las barras\n",
    "    for bar, rating in zip(bars, df_triangulation['rating_mean']):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{rating:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # L√≠nea de frecuencia \"cargador\" (NLP)\n",
    "    ax2 = ax.twinx()\n",
    "    line_nlp = ax2.plot(df_triangulation['periodo_dt'], df_triangulation['freq_cargador'], \n",
    "                        'ro-', linewidth=3, markersize=8, label='Frecuencia \"Cargador\" (NLP)', alpha=0.9)\n",
    "    \n",
    "    # L√≠nea de mediana engagement\n",
    "    ax3 = ax.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    line_eng = ax3.plot(df_triangulation['periodo_dt'], df_triangulation['votes_median'], \n",
    "                        's--', color='purple', linewidth=2, markersize=6, \n",
    "                        label='Mediana Engagement', alpha=0.8)\n",
    "    \n",
    "    # L√≠nea de sentimiento VADER (eje adicional)\n",
    "    ax4 = ax.twinx()\n",
    "    ax4.spines['right'].set_position(('outward', 120))\n",
    "    line_vader = ax4.plot(df_triangulation['periodo_dt'], df_triangulation['vader_sentiment'], \n",
    "                          '^-', color='darkblue', linewidth=2, markersize=7, \n",
    "                          label='Sentimiento VADER', alpha=0.9)\n",
    "    \n",
    "    # Configurar ejes\n",
    "    ax.set_ylabel('Rating Promedio (EDA)', fontsize=12, fontweight='bold', color='black')\n",
    "    ax2.set_ylabel('Frecuencia \"Cargador\" (NLP)', fontsize=12, fontweight='bold', color='red')\n",
    "    ax3.set_ylabel('Mediana Votos √ötiles', fontsize=11, fontweight='bold', color='purple')\n",
    "    ax4.set_ylabel('VADER Sentiment', fontsize=11, fontweight='bold', color='darkblue')\n",
    "    \n",
    "    ax.set_xlabel('Per√≠odo', fontsize=12, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelcolor='black')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    ax3.tick_params(axis='y', labelcolor='purple')\n",
    "    ax4.tick_params(axis='y', labelcolor='darkblue')\n",
    "    \n",
    "    # Formatear fechas en eje X\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%Y-%m'))\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # T√≠tulo y anotaciones\n",
    "    ax.set_title('Triangulaci√≥n Metodol√≥gica Completa: Caso Samsung A15\\n' + \n",
    "                 'EDA + NLP + Engagement + Sentiment Analysis (4 Dimensiones)', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # L√≠nea cr√≠tica en rating 3.0\n",
    "    ax.axhline(y=3.0, color='red', linestyle=':', alpha=0.7, linewidth=2)\n",
    "    ax.text(df_triangulation['periodo_dt'].iloc[2], 3.1, 'Umbral Cr√≠tico (3.0)', \n",
    "            fontsize=10, color='red', fontweight='bold')\n",
    "    \n",
    "    # Identificar convergencia m√°xima\n",
    "    pico_nlp = df_triangulation.loc[df_triangulation['freq_cargador'].idxmax()]\n",
    "    min_rating = df_triangulation.loc[df_triangulation['rating_mean'].idxmin()]\n",
    "    \n",
    "    # Anotar desfase temporal\n",
    "    if pico_nlp['periodo'] != min_rating['periodo']:\n",
    "        # Flecha mostrando desfase\n",
    "        ax.annotate('', xy=(min_rating['periodo_dt'], min_rating['rating_mean']), \n",
    "                    xytext=(pico_nlp['periodo_dt'], 3.8),\n",
    "                    arrowprops=dict(arrowstyle='<->', color='purple', lw=2))\n",
    "        ax.text(pico_nlp['periodo_dt'], 4.0, \n",
    "                f'Desfase Temporal\\nNLP: {pico_nlp[\"periodo\"]}\\nRating m√≠n: {min_rating[\"periodo\"]}', \n",
    "                ha='center', va='bottom', fontsize=9, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.8))\n",
    "    \n",
    "    # Leyenda consolidada\n",
    "    lines1, labels1 = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels() \n",
    "    lines3, labels3 = ax3.get_legend_handles_labels()\n",
    "    lines4, labels4 = ax4.get_legend_handles_labels()\n",
    "    \n",
    "    # Leyenda de intensidad\n",
    "    intensity_elements = [plt.Rectangle((0,0),1,1, color=color_map[intensity], alpha=0.8) \n",
    "                         for intensity in ['Normal', 'Baja', 'Moderada', 'Alta', 'Cr√≠tica']]\n",
    "    intensity_labels = ['Normal', 'Baja', 'Moderada', 'Alta', 'Cr√≠tica']\n",
    "    \n",
    "    legend1 = ax.legend(lines1 + lines2 + lines3 + lines4, labels1 + labels2 + labels3 + labels4, \n",
    "                       loc='upper left', fontsize=9, ncol=2)\n",
    "    legend2 = ax.legend(intensity_elements, intensity_labels, \n",
    "                       title='Intensidad Rating', loc='upper right', fontsize=9)\n",
    "    ax.add_artist(legend1)  # Mantener ambas leyendas\n",
    "    \n",
    "    # Grid y formato final\n",
    "    ax.grid(True, alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar gr√°fico\n",
    "    plt.savefig('triangulacion_metodologica_completa_4D.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return df_triangulation\n",
    "\n",
    "def analyze_methodological_convergence(triangulation_df, anomalias_rating, patrones_semanticos):\n",
    "    \"\"\"\n",
    "    An√°lisis cuantitativo de convergencia metodol√≥gica\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä AN√ÅLISIS DE CONVERGENCIA METODOL√ìGICA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    samsung_eda = anomalias_rating[anomalias_rating['producto'] == 'Samsung A15']\n",
    "    samsung_nlp = [p for p in patrones_semanticos if p['producto'] == 'Samsung A15']\n",
    "    \n",
    "    print(f\"üéØ M√âTRICAS DE TRIANGULACI√ìN:\")\n",
    "    print(f\"   ‚Ä¢ Per√≠odos analizados: {len(triangulation_df)}\")\n",
    "    print(f\"   ‚Ä¢ Rango temporal: {triangulation_df['periodo'].min()} - {triangulation_df['periodo'].max()}\")\n",
    "    \n",
    "    # An√°lisis de convergencia por criterios\n",
    "    convergencias = []\n",
    "    \n",
    "    for _, row in triangulation_df.iterrows():\n",
    "        rating_critico = row['rating_score'] >= 2  # Moderada o superior\n",
    "        nlp_negativo = row['vader_sentiment'] < -0.05  # VADER negativo  \n",
    "        vocab_cargador = row['freq_cargador'] >= 5  # Menciones significativas\n",
    "        \n",
    "        criterios_cumplidos = sum([rating_critico, nlp_negativo, vocab_cargador])\n",
    "        \n",
    "        convergencias.append({\n",
    "            'periodo': row['periodo'],\n",
    "            'rating_critico': rating_critico,\n",
    "            'nlp_negativo': nlp_negativo, \n",
    "            'vocab_cargador': vocab_cargador,\n",
    "            'total_criterios': criterios_cumplidos\n",
    "        })\n",
    "    \n",
    "    convergencias_df = pd.DataFrame(convergencias)\n",
    "    convergencias_exitosas = len(convergencias_df[convergencias_df['total_criterios'] >= 2])\n",
    "    tasa_convergencia = convergencias_exitosas / len(convergencias_df) * 100\n",
    "    \n",
    "    print(f\"\\nüìà RESULTADOS DE CONVERGENCIA:\")\n",
    "    print(f\"   ‚Ä¢ Convergencias exitosas (‚â•2/3 criterios): {convergencias_exitosas}/{len(convergencias_df)}\")\n",
    "    print(f\"   ‚Ä¢ Tasa de convergencia metodol√≥gica: {tasa_convergencia:.1f}%\")\n",
    "    \n",
    "    # An√°lisis por criterio individual\n",
    "    print(f\"\\nüîç AN√ÅLISIS POR CRITERIO:\")\n",
    "    rating_count = convergencias_df['rating_critico'].sum()\n",
    "    nlp_count = convergencias_df['nlp_negativo'].sum()\n",
    "    vocab_count = convergencias_df['vocab_cargador'].sum()\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Rating cr√≠tico (‚â•Moderada): {rating_count}/{len(convergencias_df)} ({rating_count/len(convergencias_df)*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ VADER negativo (<-0.05): {nlp_count}/{len(convergencias_df)} ({nlp_count/len(convergencias_df)*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ 'Cargador' significativo (‚â•5): {vocab_count}/{len(convergencias_df)} ({vocab_count/len(convergencias_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Identificar patrones temporales\n",
    "    print(f\"\\nüïê PATRONES TEMPORALES:\")\n",
    "    \n",
    "    # Picos de cada m√©trica\n",
    "    max_rating_score = triangulation_df.loc[triangulation_df['rating_score'].idxmax()]\n",
    "    min_rating = triangulation_df.loc[triangulation_df['rating_mean'].idxmin()]\n",
    "    max_cargador = triangulation_df.loc[triangulation_df['freq_cargador'].idxmax()]\n",
    "    min_vader = min(samsung_nlp, key=lambda x: x['avg_sentiment_vader'])\n",
    "    \n",
    "    print(f\"   ‚Ä¢ M√°ximo Score EDA: {max_rating_score['rating_score']}/4 en {max_rating_score['periodo']}\")\n",
    "    print(f\"   ‚Ä¢ Rating m√≠nimo: {min_rating['rating_mean']:.2f} en {min_rating['periodo']}\")\n",
    "    print(f\"   ‚Ä¢ Pico 'Cargador': {max_cargador['freq_cargador']} en {max_cargador['periodo']}\")\n",
    "    print(f\"   ‚Ä¢ VADER m√°s negativo: {min_vader['avg_sentiment_vader']:.3f} en {min_vader['periodo']}\")\n",
    "    \n",
    "    # Desfases temporales\n",
    "    if max_cargador['periodo'] != min_rating['periodo']:\n",
    "        print(f\"   üí° DESFASE IDENTIFICADO: Pico NLP precede rating m√≠nimo\")\n",
    "        print(f\"      ‚Üí NLP act√∫a como indicador adelantado en este caso\")\n",
    "    \n",
    "    return convergencias_df, tasa_convergencia\n",
    "\n",
    "def generate_methodological_summary(tasa_convergencia, total_periodos):\n",
    "    \"\"\"\n",
    "    Resumen metodol√≥gico del caso de estudio\n",
    "    \"\"\"\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"üéØ RESUMEN METODOL√ìGICO DEL CASO DE ESTUDIO\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"üìã DEMOSTRACI√ìN EXITOSA DE TRIANGULACI√ìN:\")\n",
    "    print(f\"   ‚úÖ Convergencia EDA-VADER: {tasa_convergencia:.1f}%\")\n",
    "    print(f\"   ‚úÖ Per√≠odos analizados: {total_periodos}\")\n",
    "    print(f\"   ‚úÖ Desfase temporal detectado: NLP adelanta rating\")\n",
    "    print(f\"   ‚úÖ Problema espec√≠fico identificado: 'Cargador' como causa\")\n",
    "    \n",
    "    print(f\"\\nüí° VALOR METODOL√ìGICO DEMOSTRADO:\")\n",
    "    print(f\"   üîπ Rating directo > scores h√≠bridos (transparencia)\")\n",
    "    print(f\"   üîπ VADER > TextBlob para espa√±ol ({tasa_convergencia:.0f}% vs 20% convergencia)\")\n",
    "    print(f\"   üîπ Triangulaci√≥n robusta en dataset peque√±o\")\n",
    "    print(f\"   üîπ NLP como indicador adelantado validado\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è LIMITACIONES RECONOCIDAS:\")\n",
    "    print(f\"   ‚Ä¢ Caso √∫nico: Samsung A15 (no generalizable autom√°ticamente)\")\n",
    "    print(f\"   ‚Ä¢ Dataset peque√±o: {total_periodos} per√≠odos analizados\")\n",
    "    print(f\"   ‚Ä¢ Contexto espec√≠fico: Mercado argentino, MercadoLibre\")\n",
    "    print(f\"   ‚Ä¢ Validaci√≥n post-hoc: Evento conocido, no predictivo\")\n",
    "    \n",
    "    print(f\"\\nüöÄ APLICABILIDAD:\")\n",
    "    print(f\"   ‚úÖ Metodolog√≠a replicable para casos similares\")\n",
    "    print(f\"   ‚úÖ Framework adaptable a otros productos/contextos\")\n",
    "    print(f\"   ‚úÖ Principios escalables con m√°s datos\")\n",
    "    print(f\"   ‚úÖ Baseline metodol√≥gico para an√°lisis futuros\")\n",
    "\n",
    "# Ejecutar triangulaci√≥n completa\n",
    "if len(df_anomalias_rating) > 0 and 'patrones_semanticos_updated' in locals():\n",
    "    print(\"üîÑ Ejecutando triangulaci√≥n metodol√≥gica final...\")\n",
    "    \n",
    "    # Crear dashboard de triangulaci√≥n  \n",
    "    triangulation_data = create_triangulation_dashboard_final(df, df_anomalias_rating, patrones_semanticos_updated)\n",
    "    \n",
    "    if triangulation_data is not None:\n",
    "        # An√°lisis de convergencia\n",
    "        convergencias_results, tasa_final = analyze_methodological_convergence(\n",
    "            triangulation_data, df_anomalias_rating, patrones_semanticos_updated\n",
    "        )\n",
    "        \n",
    "        # Resumen metodol√≥gico\n",
    "        generate_methodological_summary(tasa_final, len(triangulation_data))\n",
    "        \n",
    "        print(f\"\\n‚úÖ Triangulaci√≥n metodol√≥gica completada\")\n",
    "        print(f\"üìä Dashboard guardado: 'triangulacion_metodologica_completa_4D.png'\")\n",
    "        print(f\"üìù Metodolog√≠a: Integraci√≥n EDA + VADER + Engagement + NLP validada\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Error en creaci√≥n de dashboard de triangulaci√≥n\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Requisitos no cumplidos para triangulaci√≥n completa\")\n",
    "    print(f\"   - df_anomalias_rating: {'‚úì' if len(df_anomalias_rating) > 0 else '‚úó'}\")\n",
    "    print(f\"   - patrones_semanticos_updated: {'‚úì' if 'patrones_semanticos_updated' in locals() else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report_graphics(triangulation_df):\n",
    "    \"\"\"\n",
    "    Genera dos gr√°ficos espec√≠ficos para el informe:\n",
    "    1. Rating + \"Cargador\" (mensaje central)\n",
    "    2. Rating + VADER + Engagement (an√°lisis profundo)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "    from matplotlib.dates import DateFormatter\n",
    "    import matplotlib.dates as mdates\n",
    "    \n",
    "    # Configurar estilo para informe\n",
    "    plt.style.use('default')\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Colores por intensidad\n",
    "    color_map = {\n",
    "        'Normal': '#2E8B57',      # Verde\n",
    "        'Baja': '#FFD700',        # Dorado  \n",
    "        'Moderada': '#FF8C00',    # Naranja\n",
    "        'Alta': '#FF4500',        # Naranja rojizo\n",
    "        'Cr√≠tica': '#DC143C'      # Rojo\n",
    "    }\n",
    "    \n",
    "    colors = [color_map.get(intensidad, '#808080') for intensidad in triangulation_df['intensidad']]\n",
    "    \n",
    "    # =============================================================================\n",
    "    # GR√ÅFICO 1: RATING + \"CARGADOR\" (MENSAJE CENTRAL)\n",
    "    # =============================================================================\n",
    "    \n",
    "    fig1, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Barras de rating con colores por intensidad\n",
    "    bars1 = ax1.bar(triangulation_df['periodo_dt'], triangulation_df['rating_mean'], \n",
    "                    color=colors, alpha=0.8, label='Rating Promedio', width=25, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # A√±adir valores en las barras\n",
    "    for bar, rating in zip(bars1, triangulation_df['rating_mean']):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{rating:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    # L√≠nea de frecuencia \"cargador\"\n",
    "    ax2_1 = ax1.twinx()\n",
    "    line_cargador = ax2_1.plot(triangulation_df['periodo_dt'], triangulation_df['freq_cargador'], \n",
    "                               'ro-', linewidth=4, markersize=10, label='Frecuencia \"Cargador\"', \n",
    "                               alpha=0.9, markeredgecolor='darkmagenta', markeredgewidth=1)\n",
    "    \n",
    "    # Configurar ejes\n",
    "    ax1.set_ylabel('Rating Promedio', fontsize=14, fontweight='bold', color='black')\n",
    "    ax2_1.set_ylabel('Menciones \"Cargador\" por Per√≠odo', fontsize=14, fontweight='bold', color='darkred')\n",
    "    ax1.set_xlabel('Per√≠odo', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax1.tick_params(axis='y', labelcolor='black', labelsize=12)\n",
    "    ax2_1.tick_params(axis='y', labelcolor='darkred', labelsize=12)\n",
    "    \n",
    "    # Formatear fechas\n",
    "    ax1.xaxis.set_major_formatter(DateFormatter('%Y-%m'))\n",
    "    ax1.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, fontsize=11)\n",
    "    \n",
    "    # T√≠tulo y l√≠neas de referencia\n",
    "    ax1.set_title('Identificaci√≥n Autom√°tica del Problema del Cargador\\nCaso Samsung A15: Convergencia EDA-NLP', \n",
    "                  fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # L√≠nea cr√≠tica en rating 3.0\n",
    "    ax1.axhline(y=3.0, color='red', linestyle=':', alpha=0.8, linewidth=2)\n",
    "    ax1.text(triangulation_df['periodo_dt'].iloc[2], 3.1, 'Umbral Cr√≠tico (3.0)', \n",
    "             fontsize=11, color='red', fontweight='bold')\n",
    "    \n",
    "    # Identificar y anotar desfase temporal\n",
    "    pico_cargador = triangulation_df.loc[triangulation_df['freq_cargador'].idxmax()]\n",
    "    min_rating = triangulation_df.loc[triangulation_df['rating_mean'].idxmin()]\n",
    "    \n",
    "    if pico_cargador['periodo'] != min_rating['periodo']:\n",
    "        # Flecha mostrando desfase\n",
    "        ax1.annotate('', xy=(min_rating['periodo_dt'], min_rating['rating_mean']), \n",
    "                     xytext=(pico_cargador['periodo_dt'], 4.2),\n",
    "                     arrowprops=dict(arrowstyle='<->', color='purple', lw=3))\n",
    "        ax1.text(pico_cargador['periodo_dt'], 4.4, \n",
    "                 f'Desfase Temporal\\nNLP: {pico_cargador[\"periodo\"]}\\nRating m√≠n: {min_rating[\"periodo\"]}', \n",
    "                 ha='center', va='bottom', fontsize=11, fontweight='bold',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.9, edgecolor='orange'))\n",
    "    \n",
    "    # Leyenda\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2_1.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=12, framealpha=0.9)\n",
    "    \n",
    "    # Formato\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('informe_grafico1_rating_cargador.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # =============================================================================\n",
    "    # GR√ÅFICO 2: RATING + VADER + ENGAGEMENT (AN√ÅLISIS PROFUNDO)\n",
    "    # =============================================================================\n",
    "    \n",
    "    fig2, ax1_2 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Barras de rating TRANSPARENTES como fondo/contexto\n",
    "    bars2 = ax1_2.bar(triangulation_df['periodo_dt'], triangulation_df['rating_mean'], \n",
    "                      color=colors, alpha=0.25, width=25, \n",
    "                      edgecolor='gray', linewidth=0.8)\n",
    "    \n",
    "    # L√≠nea de VADER (protagonista)\n",
    "    ax2_2 = ax1_2.twinx()\n",
    "    line_vader = ax2_2.plot(triangulation_df['periodo_dt'], triangulation_df['vader_sentiment'], \n",
    "                            'b^-', linewidth=4, markersize=10, label='Sentimiento VADER', \n",
    "                            alpha=0.9, markeredgecolor='darkblue', markeredgewidth=1)\n",
    "    \n",
    "    # L√≠nea de engagement (protagonista)\n",
    "    ax3_2 = ax1_2.twinx()\n",
    "    ax3_2.spines['right'].set_position(('outward', 60))\n",
    "    line_engagement = ax3_2.plot(triangulation_df['periodo_dt'], triangulation_df['votes_median'], \n",
    "                                 's--', color='purple', linewidth=3, markersize=8, \n",
    "                                 label='Mediana Engagement', alpha=0.9,\n",
    "                                 markeredgecolor='indigo', markeredgewidth=1)\n",
    "    \n",
    "    # Configurar ejes\n",
    "    ax1_2.set_ylabel('Rating Promedio (referencia)', fontsize=13, fontweight='bold', color='black')\n",
    "    ax2_2.set_ylabel('Sentimiento VADER', fontsize=14, fontweight='bold', color='darkblue')\n",
    "    ax3_2.set_ylabel('Mediana Votos √ötiles', fontsize=14, fontweight='bold', color='purple')\n",
    "    ax1_2.set_xlabel('Per√≠odo', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax1_2.tick_params(axis='y', labelcolor='gray', labelsize=11)\n",
    "    ax2_2.tick_params(axis='y', labelcolor='darkblue', labelsize=12)\n",
    "    ax3_2.tick_params(axis='y', labelcolor='purple', labelsize=12)\n",
    "    \n",
    "    # Formatear fechas\n",
    "    ax1_2.xaxis.set_major_formatter(DateFormatter('%Y-%m'))\n",
    "    ax1_2.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.setp(ax1_2.xaxis.get_majorticklabels(), rotation=45, fontsize=11)\n",
    "    \n",
    "    # T√≠tulo\n",
    "    ax1_2.set_title('An√°lisis Emocional y de Respuesta Social\\nCaso Samsung A15: Dimensiones VADER y Engagement', \n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # L√≠neas de referencia\n",
    "    ax2_2.axhline(y=0, color='darkblue', linestyle=':', alpha=0.6, linewidth=2)\n",
    "    ax2_2.text(triangulation_df['periodo_dt'].iloc[1], 0.02, 'Sentimiento Neutral', \n",
    "               fontsize=10, color='darkblue', fontweight='bold')\n",
    "    \n",
    "    # Zona cr√≠tica VADER\n",
    "    ax2_2.axhspan(-0.3, -0.05, alpha=0.1, color='red', label='Zona Negativa')\n",
    "    ax1_2.grid(False)\n",
    "    ax2_2.grid(False)\n",
    "    ax3_2.grid(False)\n",
    "\n",
    "    # Identificar puntos cr√≠ticos\n",
    "    min_vader = triangulation_df.loc[triangulation_df['vader_sentiment'].idxmin()]\n",
    "    max_engagement = triangulation_df.loc[triangulation_df['votes_median'].idxmax()]\n",
    "    \n",
    "    # Anotar punto m√°s negativo VADER\n",
    "    ax2_2.annotate(f'VADER m√°s negativo\\n{min_vader[\"vader_sentiment\"]:.3f}', \n",
    "                   xy=(min_vader['periodo_dt'], min_vader['vader_sentiment']),\n",
    "                   xytext=(min_vader['periodo_dt'], min_vader['vader_sentiment'] - 0.08),\n",
    "                   arrowprops=dict(arrowstyle='->', color='darkblue', lw=2),\n",
    "                   fontsize=10, ha='center', fontweight='bold',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8))\n",
    "    \n",
    "    # Anotar engagement m√°ximo\n",
    "    ax3_2.annotate(f'Engagement m√°ximo\\n{max_engagement[\"votes_median\"]:.1f} votos', \n",
    "                   xy=(max_engagement['periodo_dt'], max_engagement['votes_median']),\n",
    "                   xytext=(max_engagement['periodo_dt'], max_engagement['votes_median'] + 2),\n",
    "                   arrowprops=dict(arrowstyle='->', color='purple', lw=2),\n",
    "                   fontsize=10, ha='center', fontweight='bold',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"plum\", alpha=0.8))\n",
    "    \n",
    "    # Leyenda consolidada\n",
    "    lines1, labels1 = ax1_2.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2_2.get_legend_handles_labels()\n",
    "    lines3, labels3 = ax3_2.get_legend_handles_labels()\n",
    "    ax1_2.legend(lines1 + lines2 + lines3, labels1 + labels2 + labels3, \n",
    "                 loc='upper left', fontsize=11, framealpha=0.9)\n",
    "    \n",
    "    # Grid sutil\n",
    "    ax1_2.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('informe_grafico2_rating_vader_engagement.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Gr√°ficos para informe generados:\")\n",
    "    print(\"   üìä Gr√°fico 1: informe_grafico1_rating_cargador.png\")\n",
    "    print(\"   üìä Gr√°fico 2: informe_grafico2_rating_vader_engagement.png\")\n",
    "    print(\"   üí° Rating transparente en Gr√°fico 2 como contexto visual\")\n",
    "\n",
    "# Ejecutar generaci√≥n de gr√°ficos para informe\n",
    "if 'triangulation_data' in locals() and triangulation_data is not None:\n",
    "    print(\"üîÑ Generando gr√°ficos espec√≠ficos para informe...\")\n",
    "    create_report_graphics(triangulation_data)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Ejecuta primero la Secci√≥n 4 completa para generar triangulation_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report_graphics(triangulation_df):\n",
    "    \"\"\"\n",
    "    Genera dos gr√°ficos espec√≠ficos para el informe:\n",
    "    1. Rating + \"Cargador\" (mensaje central)\n",
    "    2. Rating + VADER + Engagement (an√°lisis profundo)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "    from matplotlib.dates import DateFormatter\n",
    "    import matplotlib.dates as mdates\n",
    "    \n",
    "    # Configurar estilo para informe\n",
    "    plt.style.use('default')\n",
    "    sns.set_style(\"dark\")\n",
    "    \n",
    "    # Colores por intensidad\n",
    "    color_map = {\n",
    "        'Normal': '#2E8B57',      # Verde\n",
    "        'Baja': '#FFD700',        # Dorado  \n",
    "        'Moderada': '#FF8C00',    # Naranja\n",
    "        'Alta': '#FF4500',        # Naranja rojizo\n",
    "        'Cr√≠tica': '#DC143C'      # Rojo\n",
    "    }\n",
    "    \n",
    "    colors = [color_map.get(intensidad, '#808080') for intensidad in triangulation_df['intensidad']]\n",
    "    \n",
    "    # =============================================================================\n",
    "    # GR√ÅFICO 1: RATING + \"CARGADOR\" (MENSAJE CENTRAL)\n",
    "    # =============================================================================\n",
    "    \n",
    "    fig1, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Barras de rating con colores por intensidad\n",
    "    bars1 = ax1.bar(triangulation_df['periodo_dt'], triangulation_df['rating_mean'], \n",
    "                    color=colors, alpha=0.6, width=25, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # A√±adir valores en las barras\n",
    "    for bar, rating in zip(bars1, triangulation_df['rating_mean']):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{rating:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    # L√≠nea de frecuencia \"cargador\"\n",
    "    ax2_1 = ax1.twinx()\n",
    "    line_cargador = ax2_1.plot(triangulation_df['periodo_dt'], triangulation_df['freq_cargador'], \n",
    "                               'mo-', linewidth=4, markersize=10, label='Menciones \"Cargador\"', \n",
    "                               alpha=0.9, markeredgecolor='darkmagenta', markeredgewidth=1)\n",
    "    \n",
    "    # Configurar ejes\n",
    "    ax1.set_ylabel('Rating Promedio', fontsize=14, fontweight='bold', color='black')\n",
    "    ax2_1.set_ylabel('Menciones \"Cargador\" por Per√≠odo', fontsize=14, fontweight='bold', color='darkred')\n",
    "    ax1.set_xlabel('Per√≠odo', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax1.tick_params(axis='y', labelcolor='black', labelsize=12)\n",
    "    ax2_1.tick_params(axis='y', labelcolor='darkred', labelsize=12)\n",
    "    \n",
    "    # Formatear fechas\n",
    "    ax1.xaxis.set_major_formatter(DateFormatter('%Y-%m'))\n",
    "    ax1.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, fontsize=11)\n",
    "    \n",
    "    # T√≠tulo y l√≠neas de referencia\n",
    "    ax1.set_title('Identificaci√≥n Autom√°tica del Problema del Cargador\\nCaso Samsung A15: Validaci√≥n Cruzada de T√©cnicas', \n",
    "                  fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # L√≠nea cr√≠tica en rating 3.0 (alineada a la izquierda)\n",
    "    ax1.axhline(y=3.0, color='darkblue', linestyle=':', alpha=0.8, linewidth=2)\n",
    "    ax1.text(ax1.get_xlim()[1], 3.1, 'Umbral Cr√≠tico', \n",
    "         fontsize=11, color='darkblue', fontweight='bold', ha='right', va='bottom')\n",
    "    ax1.grid(False)\n",
    "    \n",
    "    # Leyenda (esquina superior derecha)\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2_1.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right', fontsize=12, framealpha=0.9)\n",
    "    \n",
    "    # Formato\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/visualizations/03_informe_grafico1_rating_cargador.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # =============================================================================\n",
    "    # GR√ÅFICO 2: RATING + AN√ÅLISIS DE SENTIMIENTO + ENGAGEMENT (AN√ÅLISIS PROFUNDO)\n",
    "    # =============================================================================\n",
    "    \n",
    "    fig2, ax1_2 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Barras de rating TRANSPARENTES como fondo/contexto\n",
    "    bars2 = ax1_2.bar(triangulation_df['periodo_dt'], triangulation_df['rating_mean'], \n",
    "                      color=colors, alpha=0.25, width=25, \n",
    "                      edgecolor='gray', linewidth=0.8)\n",
    "    \n",
    "    # L√≠nea de an√°lisis de sentimiento (protagonista)\n",
    "    ax2_2 = ax1_2.twinx()\n",
    "    line_sentiment = ax2_2.plot(triangulation_df['periodo_dt'], triangulation_df['vader_sentiment'], \n",
    "                               'b^-', linewidth=4, markersize=10, label='An√°lisis de Sentimiento', \n",
    "                               alpha=0.9, markeredgecolor='darkblue', markeredgewidth=1)\n",
    "    \n",
    "    # L√≠nea de engagement (protagonista)\n",
    "    ax3_2 = ax1_2.twinx()\n",
    "    ax3_2.spines['right'].set_position(('outward', 60))\n",
    "    line_engagement = ax3_2.plot(triangulation_df['periodo_dt'], triangulation_df['votes_median'], \n",
    "                                 's--', color='purple', linewidth=3, markersize=8, \n",
    "                                 label='Engagement (Votos √ötiles)', alpha=0.9,\n",
    "                                 markeredgecolor='indigo', markeredgewidth=1)\n",
    "    \n",
    "    # Configurar ejes\n",
    "    ax1_2.set_ylabel('Rating Promedio (referencia)', fontsize=13, fontweight='bold', color='darkgray')\n",
    "    ax2_2.set_ylabel('An√°lisis de Sentimiento', fontsize=14, fontweight='bold', color='darkblue')\n",
    "    ax3_2.set_ylabel('Engagement (Votos √ötiles)', fontsize=14, fontweight='bold', color='purple')\n",
    "    ax1_2.set_xlabel('Per√≠odo', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    ax1_2.tick_params(axis='y', labelcolor='gray', labelsize=11)\n",
    "    ax2_2.tick_params(axis='y', labelcolor='darkblue', labelsize=12)\n",
    "    ax3_2.tick_params(axis='y', labelcolor='purple', labelsize=12)\n",
    "    \n",
    "    # Formatear fechas\n",
    "    ax1_2.xaxis.set_major_formatter(DateFormatter('%Y-%m'))\n",
    "    ax1_2.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    plt.setp(ax1_2.xaxis.get_majorticklabels(), rotation=45, fontsize=11)\n",
    "    \n",
    "    # T√≠tulo\n",
    "    ax1_2.set_title('An√°lisis Emocional y de Respuesta Social\\nCaso Samsung A15: Indicadores Multidimensionales', \n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # L√≠neas de referencia (alineadas a la izquierda)\n",
    "    ax2_2.axhline(y=0, color='darkblue', linestyle=':', alpha=0.6, linewidth=2)\n",
    "    ax2_2.text(ax1_2.get_xlim()[1], 0.02, 'Sentimiento Neutral', \n",
    "           fontsize=11, color='darkblue', fontweight='bold', ha='right', va='bottom')\n",
    "    \n",
    "    # Zona cr√≠tica para sentimiento\n",
    "    ax2_2.axhspan(-0.3, -0.05, alpha=0.1, color='red', label='Zona Negativa')\n",
    "    \n",
    "    # Leyenda consolidada (esquina superior derecha)\n",
    "    lines1, labels1 = ax1_2.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2_2.get_legend_handles_labels()\n",
    "    lines3, labels3 = ax3_2.get_legend_handles_labels()\n",
    "    ax1_2.legend(lines1 + lines2 + lines3, labels1 + labels2 + labels3, \n",
    "                 loc='upper right', fontsize=11, framealpha=0.9)\n",
    "    \n",
    "   \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/visualizations/03_informe_grafico2_rating_sentiment_engagement.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Gr√°ficos para informe generados:\")\n",
    "    print(\"   üìä Gr√°fico 1: informe_grafico1_rating_cargador.png\")\n",
    "    print(\"   üìä Gr√°fico 2: informe_grafico2_rating_sentiment_engagement.png\")\n",
    "    print(\"   üí° Versi√≥n adaptada para audiencia no t√©cnica\")\n",
    "\n",
    "# Ejecutar generaci√≥n de gr√°ficos para informe\n",
    "if 'triangulation_data' in locals() and triangulation_data is not None:\n",
    "    print(\"üîÑ Generando gr√°ficos espec√≠ficos para informe...\")\n",
    "    create_report_graphics(triangulation_data)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Ejecuta primero la Secci√≥n 4 completa para generar triangulation_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Conclusiones: S√≠ntesis de Resultados Metodol√≥gicos\n",
    "\n",
    "## üìä Resultados Principales del Caso de Estudio\n",
    "\n",
    "### Convergencia Metodol√≥gica Exitosa\n",
    "\n",
    "El an√°lisis de triangulaci√≥n EDA-NLP demostr√≥ una **convergencia metodol√≥gica del 80.0%** en el caso Samsung A15, validando la efectividad del framework para datasets peque√±os.\n",
    "\n",
    "**¬øQu√© significa convergencia metodol√≥gica?** Es el grado en que t√©cnicas independientes (EDA cuantitativo, an√°lisis sem√°ntico NLP, y vocabulario cr√≠tico) coinciden en identificar los mismos per√≠odos como problem√°ticos. Una alta convergencia indica que m√∫ltiples enfoques anal√≠ticos est√°n \"viendo\" el mismo fen√≥meno, aumentando la confianza en los hallazgos cuando el dataset es peque√±o para validaci√≥n estad√≠stica robusta.\n",
    "\n",
    "**Resultados cuantitativos:**\n",
    "- **Per√≠odos analizados**: 10 (junio 2024 - marzo 2025)\n",
    "- **Convergencias exitosas**: 8/10 per√≠odos (‚â•2/3 criterios cumplidos)\n",
    "- **Performance por criterio individual**:\n",
    "  - VADER negativo (<-0.05): **90% detecci√≥n** (9/10 per√≠odos)\n",
    "  - Vocabulario cr√≠tico \"cargador\" (‚â•5): **80% detecci√≥n** (8/10 per√≠odos) \n",
    "  - Rating cr√≠tico (‚â•Moderada): **70% detecci√≥n** (7/10 per√≠odos)\n",
    "\n",
    "### Identificaci√≥n Exitosa de Problema Espec√≠fico\n",
    "\n",
    "El framework identific√≥ autom√°ticamente el **problema del cargador** como causa central de la crisis Samsung A15:\n",
    "\n",
    "- **Consistencia temporal**: Presente en 8/10 per√≠odos analizados (80%)\n",
    "- **Pico de menciones**: 31 referencias en octubre 2024\n",
    "- **Persistencia**: Problema sostenido julio 2024 - marzo 2025\n",
    "- **Especificidad**: Causa √∫nica y focalizada vs crisis multifactorial\n",
    "\n",
    "## üïê Desfases Temporales Identificados\n",
    "\n",
    "### NLP Como Indicador Adelantado Confirmado\n",
    "\n",
    "El an√°lisis revel√≥ un **patr√≥n consistente de desfase temporal** entre t√©cnicas:\n",
    "\n",
    "**Secuencia Temporal Observada:**\n",
    "1. **Julio-Septiembre 2024**: Descenso abrupto VADER (0.300 ‚Üí -0.221) anticipando crisis\n",
    "2. **Octubre 2024**: Pico vocabulario cr√≠tico (31 menciones \"cargador\")\n",
    "3. **Noviembre 2024**: Rating m√≠nimo (2.70) + Score EDA m√°ximo (4/4)\n",
    "4. **Enero 2025**: VADER alcanza punto m√°s negativo (-0.247) post-impacto\n",
    "\n",
    "**Interpretaci√≥n Metodol√≥gica:**\n",
    "- **VADER anticipa** el deterioro desde julio, detectando el problema 3-4 meses antes del m√≠nimo rating\n",
    "- **Vocabulario cr√≠tico** identifica la causa espec√≠fica 1 mes antes del impacto m√°ximo\n",
    "- **Rating confirma** el deterioro cuando ya es evidente en m√©tricas cuantitativas\n",
    "- **VADER contin√∫a** profundizando emocionalmente incluso durante la recuperaci√≥n inicial\n",
    "\n",
    "### Valor del Desfase para An√°lisis Futuro\n",
    "\n",
    "Este patr√≥n sugiere que **el an√°lisis sem√°ntico puede actuar como indicador adelantado** de deterioro reputacional, proporcionando ventana temporal para intervenci√≥n antes de que el impacto se materialice completamente en ratings promedio.\n",
    "\n",
    "## üìà Lecciones Metodol√≥gicas Clave\n",
    "\n",
    "### Fortalezas Demostradas del Framework\n",
    "\n",
    "**1. Triangulaci√≥n Robusta en Datasets Peque√±os**\n",
    "- 80% convergencia con solo 10 per√≠odos analizados\n",
    "- M√∫ltiples t√©cnicas independientes validan hallazgos\n",
    "- Reduce dependencia de validaci√≥n estad√≠stica tradicional\n",
    "\n",
    "**2. Diagn√≥stico Espec√≠fico Automatizado**\n",
    "- Identificaci√≥n de causas ra√≠z sin an√°lisis manual previo\n",
    "- Vocabulario cr√≠tico emerge autom√°ticamente del corpus\n",
    "- Especificidad que supera m√©tricas agregadas simples\n",
    "\n",
    "## üöÄ Aplicabilidad y Replicabilidad\n",
    "\n",
    "### Condiciones y Adaptaciones para Replicaci√≥n\n",
    "\n",
    "**Contextos Apropiados:**\n",
    "- Datasets peque√±os-medianos (5-50 per√≠odos temporales)\n",
    "- Productos con vocabulario cr√≠tico identificable\n",
    "- Crisis focalizadas vs difusas multifactoriales\n",
    "\n",
    "**Adaptaciones Requeridas:**\n",
    "- Calibraci√≥n de umbrales por mercado/contexto espec√≠fico\n",
    "- Selecci√≥n de herramientas NLP apropiadas para idioma target\n",
    "- Definici√≥n de vocabulario cr√≠tico relevante por industria\n",
    "\n",
    "**Extensiones Metodol√≥gicas Propuestas:**\n",
    "- Validaci√≥n cruzada con m√∫ltiples crisis documentadas\n",
    "- Sistema de alertas basado en umbral de convergencia\n",
    "- Integraci√≥n con fuentes de datos adicionales\n",
    "\n",
    "## üí° Contribuciones al Estado del Arte\n",
    "\n",
    "### Valor Metodol√≥gico del Framework\n",
    "\n",
    "**1. Triangulaci√≥n en Contextos de Datos Limitados**\n",
    "- Demuestra efectividad de validaci√≥n cruzada con muestras peque√±as\n",
    "- Principios transferibles a otras aplicaciones de an√°lisis reputacional\n",
    "- Framework flexible adaptable a diferentes contextos e industrias\n",
    "\n",
    "**2. Integraci√≥n Pr√°ctica EDA-NLP**\n",
    "- Combina simplicidad interpretativa con sofisticaci√≥n t√©cnica\n",
    "- Balance entre automatizaci√≥n y capacidad de diagn√≥stico espec√≠fico\n",
    "- Metodolog√≠a escalable manteniendo principios fundamentales\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ S√≠ntesis Final:** Este caso de estudio demuestra que la triangulaci√≥n metodol√≥gica puede extraer insights v√°lidos y accionables de datasets peque√±os, proporcionando tanto detecci√≥n de anomal√≠as como diagn√≥stico espec√≠fico de causas, siempre que se reconozcan expl√≠citamente las limitaciones contextuales y se dise√±en expectativas realistas apropiadas para el alcance disponible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GR√ÅFICO SIMPLE: RATING vs NLP (Sin Score EDA)\n",
    "# =============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def crear_grafico_rating_nlp_simple():\n",
    "    \"\"\"\n",
    "    Gr√°fico directo: Rating absoluto vs Frecuencia NLP\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configurar estilo darkgrid\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    plt.rcParams['figure.facecolor'] = 'white'\n",
    "    plt.rcParams['axes.facecolor'] = '#f0f0f0'\n",
    "    \n",
    "    # Datos Samsung EDA (solo necesitamos rating)\n",
    "    samsung_eda = df_anomalias_eda[df_anomalias_eda['producto'] == 'Samsung A15'].copy()\n",
    "    \n",
    "    # Datos NLP (frecuencia cargador)\n",
    "    samsung_nlp = [p for p in patrones_semanticos if p['producto'] == 'Samsung A15']\n",
    "    df_nlp = pd.DataFrame(samsung_nlp)\n",
    "    \n",
    "    freq_cargador = []\n",
    "    for _, row in df_nlp.iterrows():\n",
    "        vocab = row['vocabulario_critico']\n",
    "        freq = vocab.get('cargador', 0) if isinstance(vocab, dict) else 0\n",
    "        freq_cargador.append(freq)\n",
    "    \n",
    "    df_nlp['freq_cargador'] = freq_cargador\n",
    "    \n",
    "    # Merge datos\n",
    "    samsung_eda['periodo_str'] = samsung_eda['periodo'].astype(str)\n",
    "    df_nlp['periodo_str'] = df_nlp['periodo'].astype(str)\n",
    "    \n",
    "    merged_simple = pd.merge(samsung_eda, df_nlp[['periodo_str', 'freq_cargador']], \n",
    "                            on='periodo_str', how='inner')\n",
    "    merged_simple = merged_simple.sort_values('periodo').reset_index(drop=True)\n",
    "    \n",
    "    # Crear gr√°fico\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Eje izquierdo - Rating (INVERTIDO para mostrar crisis como picos hacia abajo)\n",
    "    periodos_labels = [str(p)[-7:] for p in merged_simple['periodo']]\n",
    "    x_pos = np.arange(len(periodos_labels))\n",
    "    \n",
    "    # Rating con colores seg√∫n nivel cr√≠tico\n",
    "    colors_rating = []\n",
    "    for rating in merged_simple['rating_mean']:\n",
    "        if rating < 2.8:\n",
    "            colors_rating.append('#DC143C')  # Rojo - Cr√≠tico\n",
    "        elif rating < 3.2:\n",
    "            colors_rating.append('#FF8C00')  # Naranja - Severo\n",
    "        elif rating < 3.6:\n",
    "            colors_rating.append('#DAA520')  # Dorado - Moderado\n",
    "        elif rating < 4.0:\n",
    "            colors_rating.append('#32CD32')  # Verde claro - Aceptable\n",
    "        else:\n",
    "            colors_rating.append('#228B22')  # Verde oscuro - Bueno\n",
    "    \n",
    "    bars_rating = ax1.bar(x_pos, merged_simple['rating_mean'], \n",
    "                         alpha=0.8, color=colors_rating, width=0.6, \n",
    "                         edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    ax1.set_xlabel('Per√≠odo', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Rating Promedio', fontsize=14, fontweight='bold', color='#1f4e79')\n",
    "    ax1.tick_params(axis='y', labelcolor='#1f4e79', labelsize=12)\n",
    "    ax1.set_ylim(1, 5)  # Escala completa de rating\n",
    "    \n",
    "    # L√≠nea horizontal en rating cr√≠tico\n",
    "    ax1.axhline(y=3.0, color='red', linestyle=':', alpha=0.7, linewidth=2)\n",
    "    ax1.text(len(x_pos)-1, 3.1, 'Umbral Cr√≠tico (3.0)', ha='right', va='bottom', \n",
    "             fontsize=10, color='red', fontweight='bold')\n",
    "    \n",
    "    # Eje derecho - Frecuencia NLP y Mediana Votos √ötiles\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # L√≠nea NLP\n",
    "    line_nlp = ax2.plot(x_pos, merged_simple['freq_cargador'], \n",
    "                       color='#8B0000', marker='o', linewidth=4, \n",
    "                       markersize=12, markerfacecolor='#CD5C5C', \n",
    "                       markeredgecolor='#8B0000', markeredgewidth=2,\n",
    "                       label='Frecuencia \"cargador\" (NLP)')\n",
    "    \n",
    "    # L√≠nea Mediana Votos √ötiles (Engagement)\n",
    "    line_engagement = ax2.plot(x_pos, merged_simple['votes_median'], \n",
    "                              color='#4B0082', marker='s', linewidth=3, \n",
    "                              markersize=8, markerfacecolor='#9370DB', \n",
    "                              markeredgecolor='#4B0082', markeredgewidth=2,\n",
    "                              linestyle='--', alpha=0.8,\n",
    "                              label='Mediana Votos √ötiles (Engagement)')\n",
    "    \n",
    "    ax2.set_ylabel('Frecuencia NLP / Mediana Votos √ötiles', fontsize=14, fontweight='bold', color='#8B0000')\n",
    "    ax2.tick_params(axis='y', labelcolor='#8B0000', labelsize=12)\n",
    "    \n",
    "    # Ajustar escala para acomodar ambas l√≠neas\n",
    "    max_freq = max(merged_simple['freq_cargador'])\n",
    "    max_votes = max(merged_simple['votes_median'])\n",
    "    ax2.set_ylim(0, max(max_freq, max_votes) + 5)\n",
    "    \n",
    "    # Configurar x-axis\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(periodos_labels, rotation=45, ha='right', fontsize=12)\n",
    "    \n",
    "    # Destacar convergencias: Rating bajo + NLP alto + Engagement alto\n",
    "    for i, (rating, freq, engagement) in enumerate(zip(merged_simple['rating_mean'], \n",
    "                                                       merged_simple['freq_cargador'],\n",
    "                                                       merged_simple['votes_median'])):\n",
    "        # Marcar per√≠odos de triple convergencia\n",
    "        if rating < 3.5 and freq > 10 and engagement > 5:\n",
    "            ax1.axvline(x=i, color='purple', linestyle=':', alpha=0.6, linewidth=2)\n",
    "            ax1.text(i, 4.8, '‚ö†Ô∏è', ha='center', fontsize=14)\n",
    "    \n",
    "    # Destacar desfase temporal: Pico NLP (Oct) vs M√≠nimo Rating (Nov)\n",
    "    pico_nlp_idx = merged_simple['freq_cargador'].idxmax()\n",
    "    min_rating_idx = merged_simple['rating_mean'].idxmin()\n",
    "    pico_engagement_idx = merged_simple['votes_median'].idxmax()\n",
    "    \n",
    "    # Marcar pico NLP\n",
    "    ax2.annotate('Pico Discusi√≥n\\n\"Cargador\"', \n",
    "                xy=(pico_nlp_idx, merged_simple.iloc[pico_nlp_idx]['freq_cargador']),\n",
    "                xytext=(pico_nlp_idx-0.8, merged_simple.iloc[pico_nlp_idx]['freq_cargador']+8),\n",
    "                fontsize=10, fontweight='bold', color='#8B0000',\n",
    "                arrowprops=dict(arrowstyle='->', color='#8B0000', lw=2),\n",
    "                ha='center', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='#FFE4E1'))\n",
    "    \n",
    "    # Marcar m√≠nimo rating\n",
    "    ax1.annotate('M√≠nimo\\nRating', \n",
    "                xy=(min_rating_idx, merged_simple.iloc[min_rating_idx]['rating_mean']),\n",
    "                xytext=(min_rating_idx+0.8, merged_simple.iloc[min_rating_idx]['rating_mean']-0.4),\n",
    "                fontsize=10, fontweight='bold', color='#DC143C',\n",
    "                arrowprops=dict(arrowstyle='->', color='#DC143C', lw=2),\n",
    "                ha='center', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='#FFE4E1'))\n",
    "    \n",
    "    # Marcar pico engagement si es diferente\n",
    "    if pico_engagement_idx != pico_nlp_idx:\n",
    "        ax2.annotate('Pico\\nEngagement', \n",
    "                    xy=(pico_engagement_idx, merged_simple.iloc[pico_engagement_idx]['votes_median']),\n",
    "                    xytext=(pico_engagement_idx+0.3, merged_simple.iloc[pico_engagement_idx]['votes_median']+3),\n",
    "                    fontsize=9, fontweight='bold', color='#4B0082',\n",
    "                    arrowprops=dict(arrowstyle='->', color='#4B0082', lw=1.5),\n",
    "                    ha='center', bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='#E6E6FA'))\n",
    "    \n",
    "    # Flecha mostrando desfase temporal\n",
    "    if pico_nlp_idx != min_rating_idx:\n",
    "        ax1.annotate('', xy=(min_rating_idx, 4.5), xytext=(pico_nlp_idx, 4.5),\n",
    "                    arrowprops=dict(arrowstyle='<->', color='purple', lw=3))\n",
    "        ax1.text((pico_nlp_idx + min_rating_idx)/2, 4.7, 'Desfase\\nTemporal', \n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold', color='purple')\n",
    "    \n",
    "    # A√±adir valores en puntos clave\n",
    "    for i, (rating, freq) in enumerate(zip(merged_simple['rating_mean'], merged_simple['freq_cargador'])):\n",
    "        # Mostrar valores en picos/m√≠nimos\n",
    "        if i == pico_nlp_idx or i == min_rating_idx:\n",
    "            ax1.text(i, rating + 0.1, f'{rating:.2f}', ha='center', va='bottom', \n",
    "                    fontweight='bold', fontsize=10, color='#1f4e79')\n",
    "            ax2.text(i, freq + 1, f'{int(freq)}', ha='center', va='bottom', \n",
    "                    fontweight='bold', fontsize=10, color='#8B0000')\n",
    "    \n",
    "    # T√≠tulo\n",
    "    fig.suptitle('Rating vs Discusi√≥n \"Cargador\" vs Engagement: Caso Samsung A15', \n",
    "                fontsize=16, fontweight='bold', y=0.96)\n",
    "    ax1.set_title('An√°lisis de desfases temporales entre rating, discusi√≥n sem√°ntica y engagement', \n",
    "                 fontsize=12, style='italic', pad=20, color='#555555')\n",
    "    \n",
    "    # Leyenda combinada\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#228B22', label='Rating > 4.0: Excelente'),\n",
    "        Patch(facecolor='#32CD32', label='Rating 3.6-4.0: Bueno'),\n",
    "        Patch(facecolor='#DAA520', label='Rating 3.2-3.6: Moderado'),\n",
    "        Patch(facecolor='#FF8C00', label='Rating 2.8-3.2: Bajo'),\n",
    "        Patch(facecolor='#DC143C', label='Rating < 2.8: Cr√≠tico'),\n",
    "        plt.Line2D([0], [0], color='#8B0000', marker='o', markersize=8, \n",
    "                  markerfacecolor='#CD5C5C', label='Frecuencia \"cargador\" (NLP)'),\n",
    "        plt.Line2D([0], [0], color='#4B0082', marker='s', markersize=6, \n",
    "                  markerfacecolor='#9370DB', linestyle='--', label='Mediana Votos √ötiles')\n",
    "    ]\n",
    "    \n",
    "    ax1.legend(handles=legend_elements, loc='center left', framealpha=0.95, \n",
    "              fontsize=10, fancybox=True, shadow=True)\n",
    "    \n",
    "    # Grid\n",
    "    ax1.grid(True, alpha=0.6, color='#888888', linewidth=0.8)\n",
    "    ax2.grid(True, alpha=0.3, color='#CCCCCC', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar\n",
    "    plt.savefig('rating_vs_nlp_desfase.png', dpi=300, bbox_inches='tight', \n",
    "                facecolor='white', edgecolor='none')\n",
    "    plt.show()\n",
    "    \n",
    "    return merged_simple\n",
    "\n",
    "# Ejecutar\n",
    "print(\"üìä Generando gr√°fico simple Rating vs NLP...\")\n",
    "datos_rating_nlp = crear_grafico_rating_nlp_simple()\n",
    "\n",
    "# An√°lisis del desfase temporal\n",
    "pico_nlp = datos_rating_nlp['freq_cargador'].idxmax()\n",
    "pico_engagement = datos_rating_nlp['votes_median'].idxmax()\n",
    "min_rating = datos_rating_nlp['rating_mean'].idxmin()\n",
    "\n",
    "print(f\"\\nüïê AN√ÅLISIS DE DESFASES TEMPORALES:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Pico NLP (m√°xima freq 'cargador'): {datos_rating_nlp.iloc[pico_nlp]['periodo']} \"\n",
    "      f\"({datos_rating_nlp.iloc[pico_nlp]['freq_cargador']} menciones)\")\n",
    "print(f\"Pico Engagement (mediana votos): {datos_rating_nlp.iloc[pico_engagement]['periodo']} \"\n",
    "      f\"({datos_rating_nlp.iloc[pico_engagement]['votes_median']} votos)\")\n",
    "print(f\"M√≠nimo Rating: {datos_rating_nlp.iloc[min_rating]['periodo']} \"\n",
    "      f\"({datos_rating_nlp.iloc[min_rating]['rating_mean']:.2f} estrellas)\")\n",
    "\n",
    "print(f\"\\nüìà INTERPRETACI√ìN TRIANGULAR:\")\n",
    "print(f\"‚Ä¢ Discusi√≥n 'cargador' m√°xima: {datos_rating_nlp.iloc[pico_nlp]['periodo']}\")\n",
    "print(f\"‚Ä¢ Engagement m√°ximo: {datos_rating_nlp.iloc[pico_engagement]['periodo']}\")  \n",
    "print(f\"‚Ä¢ Impacto rating m√≠nimo: {datos_rating_nlp.iloc[min_rating]['periodo']}\")\n",
    "print(f\"‚Ä¢ Patr√≥n: Discusi√≥n ‚Üí Engagement ‚Üí Impacto en Rating\")\n",
    "\n",
    "# An√°lisis adicional de convergencias\n",
    "print(f\"\\nüéØ PER√çODOS DE CONVERGENCIA M√öLTIPLE:\")\n",
    "for i, row in datos_rating_nlp.iterrows():\n",
    "    if row['rating_mean'] < 3.5 and row['freq_cargador'] > 10 and row['votes_median'] > 5:\n",
    "        print(f\"   ‚Ä¢ {row['periodo']}: Rating {row['rating_mean']:.2f}, \"\n",
    "              f\"NLP {row['freq_cargador']}, Engagement {row['votes_median']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Gr√°fico guardado como 'rating_vs_nlp_desfase.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
