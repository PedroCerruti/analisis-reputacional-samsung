import asyncio
import json
import re
from datetime import datetime
from playwright.async_api import async_playwright
import os
from pathlib import Path

# Configuraci√≥n de paths (al inicio del script)
BASE_DIR = Path(__file__).parent.parent  # Ra√≠z del proyecto (/proyecto_ml/)
DATA_RAW = BASE_DIR / "data" / "raw"
DATA_RAW.mkdir(parents=True, exist_ok=True)  # Crea la carpeta si no existe

# Configuraci√≥n
HEADLESS = False
TIMEOUT = 60000
PRODUCTOS = {
    "Motorola_G32": "https://www.mercadolibre.com.ar/motorola-moto-g32-128-gb-gris-6-gb-ram/p/MLA26924818",
    "Samsung_A15": "https://www.mercadolibre.com.ar/samsung-galaxy-a15-128-gb-negro-azulado-4-gb-ram/p/MLA32427104"
}

async def obtener_url_api(product_url):
    """Obtiene la URL API usando solo async - FUNCIONALIDAD PRESERVADA"""
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=HEADLESS)
        page = await browser.new_page()
        
        try:
            print(f"üìç Navegando a: {product_url}")
            await page.goto(product_url, timeout=TIMEOUT)
            
            # MEJORA: Verificar que la p√°gina carg√≥ correctamente
            await page.wait_for_load_state('networkidle', timeout=30000)
            
            print("üîç Buscando enlace de opiniones...")
            await page.click('a:has-text("opiniones")', timeout=10000)
            
            # Esperar la redirecci√≥n a la URL API - L√ìGICA ORIGINAL PRESERVADA
            print("‚è≥ Esperando redirecci√≥n a API...")
            async with page.expect_response(lambda r: "/catalog/reviews/" in r.url) as response_info:
                await page.wait_for_url(lambda url: "/catalog/reviews/" in url, timeout=10000)
            
            api_url = page.url
            print(f"‚úÖ API URL obtenida: {api_url[:50]}...")
            return api_url
            
        except Exception as e:
            print(f"‚ùå Error obteniendo API URL: {str(e)}")
            return None
        finally:
            await browser.close()

async def scrapear_comentarios(product_name, api_url):
    """Scrapea comentarios usando la URL API - FUNCIONALIDAD CORE PRESERVADA"""
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=HEADLESS)
        context = await browser.new_context(
            viewport={'width': 1366, 'height': 768},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        )
        page = await context.new_page()
        
        try:
            print(f"\nüîç Iniciando scrapeo de {product_name}...")
            await page.goto(api_url, timeout=TIMEOUT)

            # Esperar comentarios - L√ìGICA ORIGINAL
            try:
                await page.wait_for_selector('article[data-testid="comment-component"]', timeout=10000)
                print("‚úÖ Comentarios encontrados")
            except:
                print(f"‚ö†Ô∏è No se encontraron comentarios para {product_name}")
                return None

            all_comments = []
            
            # BUCLE PRINCIPAL PRESERVADO - Solo mejorado el logging
            for stars, rating_id in zip([5, 4, 3, 2, 1], range(1, 6)):
                print(f"   ‚≠ê Procesando {stars} estrellas...")
                try:
                    # Aplicar filtro de estrellas - L√ìGICA ORIGINAL
                    await page.click('span:has-text("Calificaci√≥n")')
                    await asyncio.sleep(1)
                    await page.click(f'[data-testid="filterItem-rating-{rating_id}"]')
                    await page.wait_for_selector('article[data-testid="comment-component"]', timeout=10000)
                    
                    # Cargar m√°s comentarios - FUNCI√ìN ORIGINAL
                    await cargar_mas_comentarios(page)
                    
                    # Extraer datos - FUNCI√ìN ORIGINAL
                    comments = await extraer_datos_comentarios(page, rating=stars)
                    all_comments.extend(comments)
                    print(f"      ‚Üí {len(comments)} comentarios extra√≠dos")
                    
                except Exception as e:
                    print(f"      ‚ö†Ô∏è Error con {stars} estrellas: {str(e)}")
                    continue

            # MEJORA: Validar que se extrajeron datos antes de guardar
            if not all_comments:
                print(f"‚ö†Ô∏è No se extrajeron comentarios para {product_name}")
                return None

            # Guardar JSON - FORMATO ORIGINAL PRESERVADO
            filename = DATA_RAW / f"comentarios_{product_name}.json"
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(all_comments, f, ensure_ascii=False, indent=2)
    
            print(f"üíæ Datos guardados en: {filename}")
            print(f"üìä Total comentarios: {len(all_comments)}")
            return filename
            
        except Exception as e:
            print(f"‚ùå Error general en scrapeo: {str(e)}")
            return None
        finally:
            await browser.close()

async def cargar_mas_comentarios(page, max_attempts=10):
    """Carga m√°s comentarios con scroll - FUNCIONALIDAD PRESERVADA"""
    attempts = 0
    last_count = 0
    
    while attempts < max_attempts:
        # Scroll - M√âTODO ORIGINAL
        await page.mouse.wheel(0, 500)
        await asyncio.sleep(1)
        
        current_count = len(await page.query_selector_all('article[data-testid="comment-component"]'))
        
        if current_count == last_count:
            attempts += 1
            # Buscar bot√≥n "Ver m√°s" - L√ìGICA ORIGINAL
            load_more = await page.query_selector('button:has-text("Ver m√°s comentarios")')
            if load_more:
                try:
                    await load_more.click()
                    await asyncio.sleep(2)
                    print(f"      üîÑ Bot√≥n 'Ver m√°s' clickeado (intento {attempts})")
                except:
                    print(f"      ‚ö†Ô∏è No se pudo clickear 'Ver m√°s' (intento {attempts})")
        else:
            attempts = 0
            last_count = current_count

async def extraer_datos_comentarios(page, rating):
    """Extrae datos estructurados - ESTRUCTURA ORIGINAL PRESERVADA"""
    comments = []
    elements = await page.query_selector_all('article[data-testid="comment-component"]')
    
    print(f"      üîé Procesando {len(elements)} elementos...")
    
    for i, element in enumerate(elements):
        try:
            # Extraer texto - SELECTOR ORIGINAL
            text_element = await element.query_selector('[data-testid="comment-content-component"]')
            if not text_element:
                continue
            text = await text_element.inner_text()
            
            # Extraer fecha - L√ìGICA ORIGINAL
            date_element = await element.query_selector('.ui-review-capability-comments__comment__date')
            if date_element:
                date = await date_element.inner_text()
                try:
                    date = datetime.strptime(date, "%d %b. %Y").strftime("%Y-%m-%d")
                except:
                    pass  # Mantener formato original si falla
            else:
                date = "N/A"
            
            # Extraer votos √∫tiles - L√ìGICA ORIGINAL
            useful_element = await element.query_selector('button[data-testid="like-button"]')
            useful_votes = 0
            if useful_element:
                useful_text = await useful_element.inner_text()
                useful_match = re.search(r"\d+", useful_text)
                useful_votes = int(useful_match.group()) if useful_match else 0
            
            # ESTRUCTURA JSON ORIGINAL PRESERVADA
            comments.append({
                "text": text.strip(),
                "rating": rating,  # Asignado desde filtro
                "date": date,
                "useful_votes": useful_votes
            })
            
        except Exception as e:
            print(f"        ‚ö†Ô∏è Error procesando elemento {i}: {str(e)}")
            continue
            
    return comments

async def main():
    """Funci√≥n principal - FLUJO ORIGINAL PRESERVADO"""
    print("üöÄ Iniciando scraper de MercadoLibre...")
    print(f"üìÅ Directorio de salida: {DATA_RAW}")
    
    for product_name, product_url in PRODUCTOS.items():
        print(f"\n{'='*50}")
        print(f"üîÑ Procesando {product_name}...")
        print(f"{'='*50}")
        
        # Obtener URL API - PASO 1 ORIGINAL
        api_url = await obtener_url_api(product_url)
        
        if api_url:
            # Scrapear comentarios - PASO 2 ORIGINAL
            result = await scrapear_comentarios(product_name, api_url)
            if result:
                print(f"‚úÖ {product_name} completado exitosamente")
            else:
                print(f"‚ùå {product_name} fall√≥ en la extracci√≥n")
        else:
            print(f"‚ùå {product_name} fall√≥ en obtener API URL")
    
    print(f"\nüéâ Scraping completado. Archivos en: {DATA_RAW}")

if __name__ == "__main__":
    asyncio.run(main())